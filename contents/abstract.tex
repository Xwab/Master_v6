% !TEX root = ../main.tex

\begin{abstract}[zh]
  %中文摘要应该将学位论文的内容要点简短明了地表达出来，应该包含论文中的基本信息，
  %体现科研工作的核心思想。摘要内容应涉及本项科研工作的目的和意义、研究方法、研究
  %成果、结论及意义。注意突出学位论文中具有创新性的成果和新见解的部分。摘要中不宜
  %使用公式、化学结构式、图表和非公知公用的符号和术语，不标注引用文献编号。硕士学
  %位论文中文摘要字数为 500 字左右，博士学位论文中文摘要字数为 800 字左右。英文摘
  %要内容应与中文摘要内容一致。

  %摘要页的下方注明本文的关键词（4 \textasciitilde{} 6 个）。
  %大语言模型在自回归生成过程中依赖KV缓存避免重复计算，但KV缓存随序列长度线性增长，带来显存占用与访存带宽的双重瓶颈，成为长上下文推理的关键限制，对KV缓存进行压缩的同时保持模型精度成为优化大模型推理的重要方向。KV缓存的规模由层数、头数、序列长度、特征维度和数据类型决定，本文主要聚焦于以低秩分解为框架的特征维度压缩。此前基于低秩的KV特征压缩方法存在一定不足，导致压缩后模型精度下降较大。首先，他们使用激活值的分布来指导KV参数的低秩分解，而V的参数在注意力层计算过程中还要与注意力分数相乘，引入注意力分数（注意力分布）能让低秩近似的优化目标更接近真实推理时的计算结果；其次，他们在模型处于未压缩状态下计算每层子空间重要性以分配不同层压缩率，这种分配方式忽略了不同层的压缩状态对彼此的影响，可能对层间子空间重要性产生误判；此外，他们根据分配到的压缩率丢弃尾部秩来实现特征降维，但V缓存的尾部秩仍对总信息量有贡献，直接丢弃会造成信息损失。为了解决上述问题，我们提出了一套包含低秩分解算法、层间秩分配策略和每层压缩方式的KV缓存特征压缩方法。我们的“注意力分布-激活”感知低秩分解算法在Value侧显式建模注意力分数对计算的影响，使低秩近似目标更贴近推理阶段真实的注意力输出；另一方面，本文提出迭代式的层间自适应秩分配策略充分考虑了不同层压缩对彼此的影响：先利用各层的奇异值能量分布进行全局初始化，再在所有层均处于压缩状态下，用校准集端到端地评估每层子空间重要性，以进行秩的迭代重分配，从而降低整体性能退化；最后，针对Value尾部秩能量小但仍有信息量的现象，我们的低秩感知量化压缩方案以“尾部低精度”的方式保留长尾信息，并与主流KV量化方法协同以支持更高压缩率。实验在Llama3.1-8B-Instruct与Qwen2.5-7B-Instruct上覆盖语言建模困惑度、零样本推理与长上下文评测。结果表明：在30\%压缩率下本文方法几乎无损；在50\%压缩率下相较现有低秩特征压缩基线取得更优的综合表现；在75\%与87.5\%高压缩率下，通过低秩感知混合精度量化仍能稳定保持推理能力。同时，显存测试显示我们的方案在推理时可显著降低KV显存占用，使模型在单卡条件下支持更长的输入上下文。综上，本文方法为大模型长上下文推理提供了一种兼具可解释性与工程可用性的KV缓存压缩方案，相较于此前的KV特征压缩方法能更好地保留模型精度。

  大语言模型在自回归生成中依赖KV缓存以避免重复计算，但其规模随序列长度线性增长，带来显存占用与访存带宽双重瓶颈，成为长上下文推理的关键限制。在压缩KV缓存的同时保持模型精度，是优化大模型推理的重要方向。KV缓存规模由层数、头数、序列长度、特征维度与数据类型共同决定，本文聚焦以低秩分解为框架的特征维度压缩。现有低秩KV特征压缩方法仍存在不足：其一，以激活分布指导参数低秩分解，而V的参数在计算过程中还要与注意力分数相乘，引入注意力分数（注意力分布）能让低秩近似的优化目标更接近注意力计算的输出；其二，在模型未压缩状态下评估层间子空间重要性并分配压缩率，忽略了层间压缩对彼此的影响，易对层间子空间重要性产生误判；其三，按压缩率直接截断尾部奇异值以降维，但Value尾部奇异值虽能量较小仍承载信息，硬截断会带来信息损失。为此，本文提出一种注意力分布和子空间重要性感知的KV缓存特征压缩方法，包含低秩分解算法、层间秩分配策略与层内压缩方式：提出“注意力分布-激活”感知低秩分解，在Value侧显式建模注意力分数影响，使近似目标更贴近推理阶段真实注意力输出；提出迭代式层间秩分配策略，先以奇异值能量分布进行全局初始化，再在全层压缩状态下利用校准集端到端评估层间子空间重要性并迭代重分配，以降低性能退化；针对Value长尾信息，提出低秩感知量化压缩，以“尾部低精度”保留长尾信息，并与主流KV量化协同以支持更高压缩率。我们在Llama3.1-8B-Instruct与Qwen2.5-7B-Instruct上进行语言建模困惑度、零样本推理与长上下文评测。结果表明：30\%压缩率下几乎无损，50\%压缩率下相较现有低秩特征压缩基线取得更优综合表现；在75\%与87.5\%高压缩率下，借助低秩感知混合精度量化仍能稳定保持推理能力。显存测试进一步显示，该方法可显著降低推理时KV显存占用，使模型在单卡条件下支持更长输入上下文。综上，本文为长上下文推理提供了一种兼具可解释性与工程可用性的KV缓存压缩方案，在保持精度方面优于既有KV特征压缩方法。

  
  
  %研究并提出了一套基于SVD的KV缓存压缩框架，以在固定显存预算下尽可能保持模型精度。具体而言，本文首先基于SVD分解提出“注意力与激活感知”的KV分解重建方法：在Value侧显式建模注意力分数对计算的影响，使低秩近似目标更贴近推理阶段真实的注意力输出，并兼容主流GQA结构下不同分组的注意力分布差异。其次，针对“整体压缩率固定时各层的秩预算分配”的问题，本文提出迭代式的层间自适应秩分配策略：先利用各层分解目标的奇异值能量分布进行全局初始化，再在所有层均处于压缩状态下，用校准集端到端地评估各层压缩误差及其累积效应，并结合秩敏感性进行迭代重分配，从而降低整体性能退化。进一步地，针对Value尾部奇异值“能量小但并非无用”的现象，本文提出低秩感知的量化方案，以“尾部低精度”的方式保留长尾信息，并与主流KV量化方法协同以支持更高压缩率。实验在Llama3.1-8B-Instruct与Qwen2.5-7B-Instruct上覆盖语言建模困惑度、零样本推理与LongBench长上下文评测。结果表明：在30\%压缩率下本文方法几乎无损；在50\%压缩率下相较现有SVD特征压缩基线取得更优的综合表现；在75\%与87.5\%高压缩率下，通过低秩感知混合精度量化仍能稳定保持长文本与推理能力。同时，显存测试显示我们的方案在推理时可显著降低KV显存占用，使模型在单卡条件下支持更长的输入上下文。综上，本文方法为大模型长上下文推理提供了一种兼具可解释性与工程可用性的KV缓存压缩方案，相较于此前的KV特征压缩方法能更好地保留模型精度。
\end{abstract}

\begin{abstract}[en]
  %Large language models (LLMs) rely on key--value (KV) caching to avoid redundant computation during autoregressive generation. However, the KV cache grows linearly with the sequence length, creating bottlenecks in GPU memory and memory bandwidth and limiting long-context inference. This thesis studies KV-cache compression from the feature-dimension perspective and proposes an SVD-based framework to better preserve accuracy under a fixed memory budget. We first propose an \emph{attention- and activation-aware} decomposition and reconstruction method, and explicitly model on the Value side how attention weights affect inference-time computation, making the low-rank objective closer to the true attention output while remaining compatible with group-wise attention differences in GQA. We then propose an iterative, layer-wise adaptive rank allocation strategy: ranks are initialized using singular-value energy profiles, and redistributed under the constraint that all layers stay compressed, guided by end-to-end error and accumulation measured on a calibration set and by rank sensitivity. Finally, since the tail singular components of Value are low-energy yet informative, we propose a low-rank-aware mixed quantization scheme that keeps the tail in \emph{low precision} and can be combined with existing KV quantization methods to support higher compression rates. Experiments on Llama3.1-8B-Instruct and Qwen2.5-7B-Instruct cover language modeling perplexity, zero-shot reasoning, and LongBench. Results show that our method is nearly lossless at 30\%, outperforms SVD-based feature compression baselines at 50\%, and remains robust at high compression rates (75\% and 87.5\%) with low-rank-aware mixed-precision quantization. Memory profiling further shows that our method enables longer input contexts on a single GPU. Overall, the proposed approach is an interpretable and practical KV-cache compression solution that preserves accuracy better than prior KV feature compression methods.
  Large language models (LLMs) rely on key–value (KV) caching during autoregressive generation to avoid redundant computation. However, the KV cache grows linearly with sequence length, creating dual bottlenecks in GPU memory footprint and memory bandwidth, and thus becoming a key constraint for long-context inference. Compressing the KV cache while preserving model accuracy is therefore an important direction for improving LLM inference efficiency. The KV cache size is jointly determined by the number of layers, the number of heads, the sequence length, the feature dimension, and the data type; this work focuses on feature-dimension compression under a low-rank decomposition framework. Existing low-rank KV feature compression methods still suffer from several limitations. First, they guide low-rank decomposition using activation distributions, yet the Value (V) parameters are further multiplied by attention scores during attention computation; incorporating attention scores makes the low-rank approximation objective closer to the attention outputs. Second, they estimate layer importance and allocate layer-wise compression ratios in the uncompressed model, ignoring inter-layer interactions under compression and potentially misestimating layer importance. Third, they reduce dimensionality by directly truncating tail singular values according to the target compression ratio, but although the tail singular values of Value have low energy, they still carry information; hard truncation therefore causes information loss. To address these issues, we propose an attention-distribution- and subspace-importance-aware KV-cache feature compression method that integrates a low-rank decomposition algorithm, an inter-layer rank allocation strategy, and a layer-wise compression scheme. We introduce an attention–activation-aware low-rank decomposition that explicitly models the effect of attention scores on the Value side, aligning the approximation objective more closely with the true attention outputs during inference. We further propose an iterative inter-layer rank allocation strategy: it first performs a global subspace-dimension initialization based on singular-value energy distributions, then evaluates layer importance end-to-end on a calibration set while all layers remain compressed, and iteratively reallocates subspace dimensions to mitigate overall performance degradation. Finally, to preserve long-tail information in Value representations, we propose a low-rank-aware quantization scheme that retains tail singular values in low precision and works in conjunction with mainstream KV quantization methods to support higher compression ratios. We evaluate our approach on Llama3.1-8B-Instruct and Qwen2.5-7B-Instruct across language-modeling perplexity, zero-shot reasoning, and long-context benchmarks. Results show that our method is nearly lossless at a 30\% compression ratio, achieves better overall performance than existing low-rank feature-compression baselines at 50\% compression, and remains stable at high compression ratios of 75\% and 87.5\% with the help of low-rank-aware mixed-precision quantization. GPU memory measurements further demonstrate that our method substantially reduces KV-cache memory usage during inference, enabling longer input contexts on a single GPU. Overall, our method provides an interpretable and practical KV-cache compression solution for long-context LLM inference, preserving accuracy better than prior KV feature compression approaches.
\end{abstract}
