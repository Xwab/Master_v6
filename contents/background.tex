\chapter{相关工作}
本章围绕 KV 缓存压缩的既有研究展开综述，依次从序列维度、层和头维度、数据类型（量化）与特征维度四个角度梳理代表性方法与最新进展。首先回顾通过滑动窗口、固定位置保留和根据注意力分数丢弃词元等方式直接缩短序列长度的方案；随后介绍层级共享和头间合并等压缩的思路；继而讨论不同精度和粒度量化在KV矩阵上的实践；最后聚焦于依托特征维度低秩与冗余性质进行特征维度压缩的技术路线。通过这四条主线，本章为后续章节提出的特征维压缩及其协同策略奠定参考背景。

\section{序列维度KV缓存压缩}
在 KV 缓存压缩策略中，序列维度压缩直接作用于历史 token 的数量，通过“少存一些 token”来线性降低缓存规模。序列维度压缩KV的策略可以系统地分为三类：其一是基于位置的压缩，借助滑动窗口保留最近位置的词元或保留固定位置的词元。Longformer \cite{beltagy2020longformer}使用滑动窗口的策略，只保留最近的词元。StreamingLLM \cite{xiaoefficient} 发现除了最近位置的词元之外，KV缓存中还存在同样重要的"sink词元"，位于序列的开头，这些词元对模型的效果影响很大。最近的词元与当前词元语义关系更相近，而开头的词元主要作用是收集多余的注意力分数，这是因为在训练的时候由于自回归模型的顺序性序列初始的词元是全序列可见的，StreamingLLM通过仅保留最近和开头词元这两部分固定位置的KV缓存实现压缩。LM-Infinite\cite{han2024lm}和StreamingLLM较为类似，也保留了序列开头的词元和最近的词元，但其保留最近词元的数量为模型预训练时的序列长度，并且序列开头词元表示位置信息的位置编码和最近邻词元的左端一致。仅基于位置的压缩方式缺点在于不够灵活，对于模型推理阶段的不同词元都使用相同的位置策略保留KV缓存。

在大模型的生成过程中，后续的不同词元的会固定对其之前不同的一小部分词元保持高相关度，仅靠位置进行丢弃词元可能丢掉了对当前词元重要的信息，因此在此基础上也有许多基于KV缓存内容的压缩策略。这类策略通过词元的注意力权重，语义信息相似度等指标来挑选词元，实现动态灵活的丢弃或合并。H2O \cite{zhang2023h2o}对KV缓存中的每个词元计算了其后续所有词元对其的注意力分数之和，当需要在KV缓存中淘汰词元时，选择累积注意力分数最小的词元淘汰，并且也保留了相对重要的所有最近邻的词元。Scissorhands \cite{liu2023scissorhands} 进一步将累积注意力分数二值化，在KV缓存中保留二值化后累积注意力分数最大的词元和生成端最近邻的语义相关词元。TOVA \cite{oren2024transformers} 认为模型生成阶段相邻的词元的Query之间对KV缓存中不同词元的关注程度有着相似性。因此对每个生成的词元，使用其上一个生成词元的Query的注意力分数作为丢弃KV缓存中词元的标准，淘汰那些上一个词元最不关注的内容。SnapKV \cite{li2024snapkv} 在模型预填充阶段进行一次性的KV缓存压缩。在预填充阶段结束后，SnapKV将KV缓存中的词元划分为最近词元和待压缩词元两部分，计算每个最近词元对待压缩词元的注意力，对每个待压缩部分中的词元，取这些注意力中的最大值作为丢弃词元的指标，根据压缩率丢弃指标最小的词元。PyramidKV \cite{cai2024pyramidkv}考虑到在不同层之间分配不同的压缩预算，淘汰不同数目的词元来在总预算固定的情况下达到更优的效果。他观察到随着模型层级的加深，注意力分数逐渐由分散变得集中，不同的词元都将注意力聚焦在一小部分词元上，因此PyramidKV提出了可以对不同层保留不同长度的KV缓存，浅层保留更多的词元，压缩率更低，而深层则可以丢弃KV缓存中更多的词元。在每层内部其压缩方式与SnapKV相同。PyramidInfer \cite{yang2024pyramidinfer}发现随着模型层数加深，仅压缩单层对模型效果的影响有着变小的趋势，其思路与PyramidKV相似，也是随着层数加深逐渐增大压缩率。在每层内部，丢弃词元的方式与SnapKV相似，取每个最近词元对待压缩词元注意力的均值而非最大值作为淘汰KV对的指标。AdaKV\cite{feng2024ada}进一步考虑到了对于不同注意力头的压缩预算分配。AdaKV将同一层所有注意力头的注意力权重拼接为一个序列，从拼接的序列中取最大的一部分，统计每个头被选中的权重数量，并根据此为每个注意力头分配压缩预算。AdaKV还发现同一层中有的注意力头的注意力权重更集中，有些更分散，在用上述策略分配每个头的预算后再依据集中还是分散型头进行压缩预算调整。FastKV \cite{jo2025fastkv}发现在大模型较浅的层中，不同层之间关键词元（即累积注意力高的词元）的重合较少，注意力模式不稳定，需要处理完整上下文来捕获完整的依赖关系；随着模型层数加深，注意力模式逐渐稳定，不同层之间重要词元相对固定，因此可以只将这部分的重要的词元传播给深层，这既压缩了深层的KV缓存，也减少了预填充阶段的计算量，加速了模型推理。FastKV先通过实验找到注意力模式分散和稳定的分界层，对于分界层之前的浅层，对所有的词元进行计算，并根据最近窗口内词元对待压缩部分词元的注意力分数进行选择丢弃；对于分界层之后的深层，根据最近窗口词元对其他词元的注意力分数大小选出一部分重要的词元，只向后续层传播这部分词元，并且也根据同样的标准丢弃词元以进一步压缩KV缓存。ChunkKV\cite{liu2025chunkkv}认为之前的方法通常基于单个词元的重要性进行淘汰，忽略了词元之间语义关联性带来的影响，这可能导致上下文信息被碎片化丢弃，从而损害模型的理解能力。为了解决这个问题，ChunkKV将序列按顺序划分为若干个组块，计算最近邻窗口的词元对前序每个词元的注意力分数，并将每个组块内所有词元的注意力分数求和作为该块的重要性指标。压缩KV缓存时根据压缩率，只保留最近邻的词元和重要性最高的组块。ChunkKV还将层分成了若干组，根据相邻层之间的相似性，对每组的不同层都保留相同位置的组块。

虽然KV缓存的不同词元存在一定冗余信息，但其中冗余的部分在模型生成过程中并不固定，采用完全丢弃词元的方式加剧了丢失重要信息的风险。因此一些研究使用合并词元代替丢弃词元的方式来从序列维度压缩KV缓存，以期保留更多的信息。CaM\cite{zhang2024cam}发现Value的冗余性比Key要小，因此采取合并Value的方式对V缓存压缩。在StreamingLLM的基础上，CaM将本来要丢弃的Value有一定概率地以$\frac{1}{n}$的权重合并到其后续n个词元的Value上，并且词元的累积注意力分数越高，这个概率就越大。WeightedKV\cite{yuan2025weightedkv}同样采取了对K缓存中词元丢弃，对V缓存中词元合并的压缩策略。对于累积平均注意力分数最小的词元的Key向量，WeightedKV直接将其丢弃；对于Value向量，WeightedKV将其和该词元右侧的Value向量以凸组合的形式合并，系数为两个词元各自的归一化后的累积平均注意力分数。Lococo\cite{cai2024lococo}引入了另一个独立的可训练的卷积神经网络来计算不同词元的Key缓存和Value缓存合并时的权重。该研究将缓存中的KV对（长度为M）和最新输入的一部分KV对（长度为B）拼接在一起，然后通过可训练的卷积层。卷积层的输出为当前缓存中的词元和最新输入的词元的权重（$M\times (M+B)$），每个当前缓存中词元有一份自己的权重。压缩后KV缓存中每个词元的KV向量是使用卷积得到的权重对当前缓存中KV以及最新输入词元的KV分别进行加权和。


\section{模型层维度和注意力头维度压缩}
除了序列维度之外，KV 缓存还在不同模型层之间以及同层各注意力头之间包含冗余信息。因此除了在序列方向丢弃或合并词元的KV，还可以进一步从层级结构和注意力头维度入手，通过挖掘跨层共享模式、头间共享关系等方式对KV进行压缩。考虑到相邻层之间KV缓存的相似性，CLA\cite{brandon2024reducing}设计了一种跨层的注意力计算方式，即相邻层之间共享Key和Value，但Query不进行共享。CLA能够将KV缓存压缩等于层间共享因子（即多少层之间共享KV）的倍数。CLLA\cite{yang2024lossless}使用了MLA\cite{liu2024deepseek}（Multi-head Latent Attention）的思想，将每个词元的输入通过同一个参数矩阵投影得到一个KV共享的表示C，来替代KV缓存，再通过Key和Value各自的新的投影矩阵与C相乘得到参与注意力计算的KV。CLLA与CLA的主要区别在于其共享的是隐藏状态C而不是KV，这能够在共享每层缓存的同时也给了不同层各自的不同表示能力。LCKV\cite{liu2024lckv}则是考虑到大模型最深层的表征是最具有信息量的，因为其包含了每一个前序层的信息。因此其仅使用最深一层的KV缓存以此来压缩整体的KV缓存体积。

注意力头之间也可以共享KV缓存，但这与最初的多头自注意力机制MHA每个词元的Query-Key-Value在每个注意力头一一对应的结构不同，是对大模型基础架构的改动，因此一般需要重新对模型进行预训练。MQA（Multi-Query Attention）\cite{shazeer2019fast}中舍弃了Key和Value按注意力头划分的机制，所有的Query头对应同一个KV，但这一定程度上弱化了模型的表达能力，并且影响了模型的稳定性。在MQA的基础上，GQA（Grouped-Query Attention）\cite{ainslie2023gqa}采用了更加折中且合理的方案。GQA与MHA相比Query的头的数量不变，但把这些头按组共享同一个K/V。常见做法是将$n_q$个Query头对应$n_{kv}$个个Key/Value头，每$\frac{n_q}{n_{kv}}$个Query头共享同一个KV。因为每个KV组只需要存一次KV，GQA实现了对KV缓存按组数成比例压缩，并且也很大程度上保留了原本MHA的表示能力，在Llama \cite{dubey2024llama}, Qwen\cite{team2024qwen2}等许多大模型中都使用了GQA的架构。

\section{数据类型角度压缩}
前文分别讨论了从序列角度丢弃或合并词元，从层或头的角度进行KV缓存的共享来压缩KV缓存的方式，其底层数据类型仍然是以高精度存储在缓存中的。本小节将从数据类型的维度，通过量化等低比特表示来限制单个元素的存储成本。这类方法不改变KV缓存的形状和数量，而是用更紧凑的数值格式（如对称或非对称量化，混合精度量化等）来编码KV缓存，从而在保持注意力行为基本稳定的前提下进一步压缩 KV，并为大批量推理提供额外的显存余量。量化方式可以分为对称量化和非对称量化：对称量化：设需要转换数据类型的矩阵为$X$，首先计算一个缩放系数$s=\frac{\text{absmax(X)}}{q_{max}}$，其中$q_{max}$为低精度数据类型的最大值，通过$s$来将原本$X$中的数值范围映射到目标低精度数据类型范围内，再通过取整得到量化后的结果$X_q=\text{round}(\frac{X}{s})$，在推理时用量化后得到的$X_q$再通过系数$s$进行去量化的操作得到原本数据类型的矩阵$\hat{X}=X_Q \cdot s$。对称量化比较适用于数据分布对称的场景。非对称量化则是额外考虑了零点$z$以及目标低精度数据类型的最小值$q_{min}$。因此缩放系数的计算方式变为了$s=\frac{X_{max}-X_{min}}{q_{max}-q_{min}}$，零点的计算方式为$z = \text{round}(q_{min}-\frac{X_{min}}{s})$。因此量化后低精度数据类型的目标矩阵为$X_q=\text{round}(\frac{X}{s})$，去量化的计算方式为$\hat{X}=(X_Q-z)\cdot s$。相比对称量化，非对称量化对正负分布有一定偏移的数据能更好地拟合，但需要额外存储一个零点$z$，并且去量化时也要额外用$z$参与计算。在用量化的方法压缩KV缓存时经常遇到离群值的问题，即KV缓存中存在的少量幅值远超其他正常值的元素。离群值会对系数$s$的计算产生影响，导致许多本该被映射到不同低比特数值的正常值被映射到了相同的数值，进行去量化还原为原本数据类型时分辨率降低使得模型性能下降。QServe\cite{linqserve}发现Key矩阵中存在比较明显的奇异值，而Value中并不是很明显，因此只对Key进行了处理。QServe对Key矩阵进行了逐个通道的放缩，相当于对其乘一个对角矩阵$D=\text{diag}(\lambda)^{-1}$，其中$\lambda_i=\text{max}(|K_i|^\alpha)$，$i$表示K矩阵的第i个通道，并将逆变换的$D^{-1}$融入Query矩阵中，使得Q与K计算注意力时结果不变：注意力$A=(QD^{-1})(KD)^T$。Quarot\cite{ashkboos2024quarot}是一个量化模型参数和激活值的方法，他提出通过对待量化的矩阵进行Hadamard变换能够消除矩阵中的离群值。Hadamard变换相当于对原矩阵乘以一个特殊的正交阵，并且存在快速的Hadamard变换算子实现，可以快速计算。这个方法对KV缓存的量化压缩同样适用，先对KV缓存进行Hadamard变换再进行量化，再注意力层计算时再进行去量化和Hadamard的逆变换。ATOM\cite{zhao2024atom}动态识别并重排激活值中的异常通道，将其保留为INT8精度，正常值量化为INT4，兼顾精度与效率；并且将重排序、量化、反量化操作融合到GEMM计算流水线中，减小了额外开销。KIVI\cite{liu2024kivi}基于Key矩阵和Value矩阵异常值的不同特点，对KV采取了不同粒度的量化方案。对于Key缓存，某些特定的通道会持续出现数值幅度异常大的离群值，采用通道级量化以限制误差传播；对于Value缓存，虽然异常值不多，但由于注意力稀疏性，使用词元级量化来更好保持重要词元的精度。

KV缓存中存在信息的冗余，有些不重要的或不存在离群值的KV可以采用更低精度量化，这种混合精度的量化方式使模型能在更大的压缩率下保持性能基本不变。KVQuant\cite{hooper2024kvquant}系统分析了KV缓存的分布特性，提出预RoPE通道级Key量化以更好处理异常值；引入了敏感度加权的非均匀量化方法，显著提升低比特下的表示精度，设计了每向量稠密-稀疏混合存储策略，有效平衡压缩率与模型性能。QAQ\cite{cheng2025qaq}同样观察到Key和Value对量化表现出不同的敏感性，需要分别制定量化策略。QAQ设计了注意力窗口机制，通过跟踪历史注意力分数最大值，预测未来分数变化，处理注意力重要性持续性假设的例外情况。并且也将异常值保留为全精度，正常值进行低比特量化。LogQuant\cite{hanlogquant}基于对注意力峰值的位置遵循对数分布，随着词元远离当前位置而变得稀疏这一现象的观察，创新地提出了一种对数分布式的KV缓存量化方案。其维护最近的一部分词元的KV为全精度，对于早期词元进行对数间隔采样，采用到的词元KV保留全精度，其余词元的KV量化为2-bit。

在低比特量化中，2-bit量化方案已经能在极端压缩与可用精度之间取得平衡，但也有一些方法采用了更激进的1-bit量化，把 KV 缓存逼近二值化极限，并设计了全新的编码策略或误差补偿机制来维持模型的表现。CQ\cite{zhang2024kv}使用通道耦合编码来实现1bit的量化。比如其中的一种设置是连续的8个通道共享一个8位的码本，等效于每个通道的值可以仅用1bit来表示。CQ基于的信息论理论基础是多个通道的联合熵增长慢于其边际熵之和，这意味着耦合编码比独立编码更信息高效，特别是在低比特量化时优势更明显。QJL\cite{zandieh2025qjl}对Key使用1bit量化，对Value使用多bit量化。对Key矩阵，先用Johnson-Lindenstrauss变换进行随机高斯投影，再只取投影结果的符号来进行1bit量化。文中证明，同样对Query进行Johnson-Lindenstrauss变换之后与1bit的Key相乘是对原本全精度注意力的一个无偏估计。对于Value矩阵的则是使用了常规的词元级别的量化方式。NSNQuant \cite{son2025nsnquant}使用采用三重变换架构（Normalize-Shift-Normalize）实现对KV缓存的无校准量化：先用词元级归一化，抑制离群值的影响；再使用通道级中心化，消除均值偏移；最后再次用词元级归一化，标准化输出分布。NSNQuant结合Hadamard变换，证明经过NSN处理后的通道分布收敛于标准正态分布，从而可预先构建针对标准正态分布的通用码本实现1bit量化，彻底摆脱对校准数据的依赖。

\section{KV缓存特征降维压缩}
序列维度，层与头结构的设计，改变数据类型精度（量化）来压缩KV缓存都是在“既定特征表示”不变的前提下降低存储/带宽，而每个KV向量的特征表示其实也存在冗余和不重要的信息。因此，可以直接针对 KV 的特征维度做降维，能从源头减少每个 token 需要保留的向量中的冗余信息。特征维度压缩KV缓存的方案一般可以分为两种：第一种是对参数矩阵$W_k$和$W_v$进行低秩分解，得到两个低秩的矩阵相乘：$W_k=A_kB_k$，$W_v=A_vB_v$，再用输入的激活值与分解得到的$A$矩阵相乘得到降维之后的中间值，通过存储中间值来代替对原本KV缓存的存储实现降维；另一种方案是对KV缓存乘一个低维的投影矩阵来降低其特征维度。这两种方式在注意力层计算时，都需要对存储的降维后的KV缓存再进行一次变换，低秩分解的方式是需要乘以分解得到的另一个矩阵$B$，低维投影的方式则是需要再乘以一个升维的逆变换矩阵。

对参数矩阵低秩分解实现特征降维的方法一般基于奇异值分解（SVD）。SVD将任一矩阵$A\in \mathbf{R}^{m \times n}$拆成$U\Sigma V$，其中$U$和$V$为正交矩阵，$\sigma$为按降序排列的奇异值对角矩阵。按能量截断 $\sigma$ 中最大的一部分奇异值（比如前a个），就得到逼近 $A$ 的低秩矩阵 $\hat{A} = U_a\Sigma_aV_a^T$。当矩阵$A$本来就有低秩性，即奇异值能量主要集中在一小部分奇异值上的时候，截断一部分奇异值不会对矩阵的Frobenius范数有大的影响。ASVD\cite{yuan2023asvd}在进行SVD分解时考虑到了激活值与参数矩阵相乘带来的影响，认为SVD分解低秩近似导致的误差会被激活值幅值更大的通道放大，会对压缩后模型效果有较大影响。因此先对参数矩阵乘以一个由激活值每个通道平均幅值构成的对角阵再进行SVD，并对SVD后的结果乘以对角阵的逆矩阵来保持运算结果不变。LORC\cite{zhang2024lorc}同样基于SVD，考虑到大模型不同层对于低秩近似的敏感度不同，提出了基于条件数来动态分配每一层的压缩率。文中定义了每一层的输出在对该层进行低秩近似之后的相对变化量的上界该层的条件数乘以该层输入的相对变化量，条件数越大的层对其前一层分解带来的该层输入的变化就越敏感。因此LORC根据每一层的累积条件数分配保留的秩数量，即分配每层压缩率。CSKV\cite{wang2024cskv}基于ASVD的低秩近似方式，结合了序列维度不同词元有不同重要性的想法，对于最新的词元使用完整参数矩阵计算得到的KV缓存，对于过去的词元则是用SVD存储低秩中间值的方式进行降维。并且CSKV还对ASVD分解得到的低秩矩阵进行了微调，损失函数为降维再重建之后近似的KV缓存与完整参数矩阵计算的KV缓存之间的差异。Palu\cite{chang2025palu}对激活值的影响进行了进一步推导，通过求解SVD前后的参数矩阵与激活值相乘的结果做差的Frobenius范数最小的优化问题，推导出在SVD之前需要先对参数矩阵乘以白化矩阵。并且他还发现对参数矩阵的逐个注意力头分解效果相比对所有注意力头联合分解效果更差，但是重建速度更快。为了同时兼顾计算速度和压缩之后的模型效果，Palu将参数矩阵的注意力头分组，每组包含多个注意力头并在组内联合分解。文中还对KV特征降维后的模型进一步应用了低比特量化来提高模型的压缩率。Palu基于费希尔信息量（Fisher Information）来分配每一层要保留多少秩。在一部分验证集上计算每一层参数矩阵$W_k$和$W_v$的费希尔信息量，越大说明该层对模型参数变化越敏感，需要分配更多的秩。ALRD在激活值的基础上进一步考虑了模型推理时Value矩阵还需要和注意力分数$A$相乘，因此对$W_v$求解的优化问题变成了低秩近似前后$AXW_v$相乘的结果差异尽可能小，得到了在进行SVD之前需要对$W_v$进行与$W_k$不同的变换。并且在每层的秩分配时，基于费希尔信息量，相比Palu更显示地考虑了每一层的每一个秩是否舍弃对模型效果产生的影响，相较于Palu有较大的效果提升。

基于降维投影压缩KV缓存特征维度的方法一般是对原本的特征维度较大的KV缓存乘以一个低维的投影矩阵，在计算注意力的时候再乘以升维的逆变换矩阵维持计算不变的性质。 Eigen Attention \cite{saxena2024eigen}在一部分验证集上计算Query，Key和Value向量，对Key和Value的投影矩阵分别通过对验证集上的QK拼接和对验证集上的V自身进行SVD分解得到。除了SVD之外，PCA主成分分析（Principal Component Analysis）也是一种常用的降维手段，其核心目标是通过线性变换将高维数据投影到低维空间，同时尽可能保留信息的方差。具体过程包括：首先对样本矩阵$X$进行零均值化，计算协方差矩阵$C=\frac{1}{n}X^TX$并求解其特征值与特征向量；然后按照特征值大小选择前$r$个主成分构成投影矩阵$P_r$，用$\tilde{X}=XP_r$得到降维表示；在需要恢复到原特征维度时，再通过$X_{\text{rec}}=\tilde{X}P_r^T$（或结合保留下来的均值向量进行平移）来重建近似的原始数据。MatryoshkaKV\cite{linmatryoshkakv}使用PCA投影矩阵来初始化降维矩阵，并在此基础上对降维矩阵进行微调。为了保持投影矩阵的正交性，使得重建后的KV缓存能更好地近似原本的完整KV，MatryoshkaKV使用了Cayley参数化。OjaKV\cite{zhu2025ojakv}则在此基础上引入动态自适应PCA，将“首段+最近”词元保留全秩、其余词元低秩化的混合缓存策略与在线子空间更新结合：预填充阶段参考SnapKV式注意力打分挑选少量关键词元执行Oja增量更新，解码阶段周期性对压缩基进行轻量刷新，使投影子空间始终贴合随上下文漂移的数据分布。该方法在缓存端保存压缩后的$\tilde{K},\tilde{V}$，调用FlashAttention前再即时重构，从而保持良好的工程兼容性；并且由于只对中间词元执行低秩投影，可与序列维度方法叠加，获得乘法式显存节约。

与前文提到的低秩分解和降维投影不同，DeepSeek系列模型\cite{liu2024deepseek}提出的原生MLA（Multi-Head Latent Attention）从注意力计算的架构上进行更改，同样实现了特征维度压缩KV缓存的目的。MLA首先把词元隐状态通过统一的编码矩阵$W_e$映射为低维潜变量$C\in\mathbb{R}^{n\times d_{\text{lat}}}$，仅缓存$C$与位置编码，以这样的方式实现了KV缓存的统一和压缩；随后每个注意力头$i$再使用极小的头内矩阵$W_{k,i},W_{v,i}$从$C$恢复出该头的$K_i,V_i$，并在RoPE之后参加常规注意力计算。因为$d_{\text{lat}}\ll h\cdot d_{\text{head}}$，缓存和带宽消耗近似按压缩比$\frac{d_{\text{lat}}}{h\cdot d_{\text{head}}}$缩减；同时，MLA还为每个头保留可训练的门控/残差路径，确保不同头之间仍具备差异化表达能力。MLA相较传统MHA几乎无精度回退，却能大幅降低KV缓存开销。

\section{本章小结}
综上所述，序列裁剪、层/头共享、数据类型压缩与特征维度降维分别从不同维度削减KV缓存，但它们并非互斥：前两类方法主要减少需要存储的词元或层级、头数量；量化进一步压缩单元素的比特宽度；特征维度压缩则直接缩短每个KV向量的长度，能够与前述手段叠加获得更多收益。其中，基于低秩性质的特征维压缩有较好的理论可解释性：一方面，Transformer激活和注意力矩阵普遍存在能量集中、跨通道冗余等特征，使得低秩近似可以在可控误差下实现压缩；另一方面，低秩/投影方法对推理流水线较为透明，既可通过矩阵分解存储中间值，也可通过外部投影矩阵直接压缩缓存，便于与FlashAttention、量化等技术组合。本论文正是基于这一方向，聚焦于如何依托SVD低秩分解实现特征维的压缩。

%然而现有工作仍存在研究空间：静态投影在跨域迁移和动态上下文下易失效，在线子空间更新如何兼顾稳定性与计算开销仍未定论；大规模模型的不同层、不同注意力头在低秩敏感度上并不一致，压缩预算的细粒度分配仍缺乏系统方法；低秩压缩与量化/稀疏化之间的误差叠加机制尚待深入分析；此外，如何在保持精度的前提下进一步降低重建带宽、减少额外矩阵乘法，也是工程部署中的关键问题。鉴于此，后续章节将围绕“特征维度低秩压缩+自适应重建”展开研究，探索更高效的子空间追踪、误差补偿与多策略协同方案。

然而现有工作仍存在研究空间：首先，现有基于SVD的特征降维方法的优化目标往往只关注激活值带来的误差放大，尚未系统刻画注意力分数$A$对Value低秩近似误差的影响；在实践中，低秩近似若忽略注意力分数的分布，可能导致注意力层的输出误差被放大。其次，在根据每层的重要程度进行秩分配时，需要考虑其他层压缩对目标层的影响：前序层压缩对目标层压缩误差的影响和误差在后续层不断累积的效应，以及各层本身参数矩阵自身的低秩性质。除此之外，由于Value与Key的低秩性并不完全相同，Value的尾部秩对总信息量仍然有贡献，现有方法中直接“非1即0”地截断丢弃尾部奇异值将造成一定信息损失。
%再次，低秩压缩与量化叠加时会出现误差耦合：投影基的宽度决定了后续可用的量化动态范围，而若所有秩统一使用相同比特宽度，主导奇异值方向上的重要信息容易被低精度噪声淹没。
鉴于此，后续章节将围绕“显式建模注意力和激活值影响的低秩近似算法+模型各层压缩状态下基于层敏感性和误差累积的迭代式层间秩分配策略+基于秩重要程度的低秩感知混合精度量化方案”展开研究。