% !TEX root = ../main.tex

\chapter{绪论}
\label{chap:intro}


\section{研究背景}
大语言模型(Large Language Model)近几年来发展迅猛，成为自然语言处理领域的核心技术力量。因为其强大的自然语言理解与生成能力，丰富的跨领域知识以及对用户个性化的需求的良好适配，大模型已经在智能问答、代码生成、信息检索、多语种翻译、数据分析助手、以及跨模态内容创作等场景中广泛应用，并成为企业知识库、智能客服和自动化研发流程的基础。大语言模型的规模巨大，顶尖的模型参数量已经达到了数百亿甚至千亿的级别。这种超大规模的参数量使得模型能够捕捉到语言中更加细微的模式和规律，提供更加智能和精准的结果。同时，模型通过海量的无标记数据进行自监督学习，利用大数据集从中学习语言的结构和知识，从而增强了模型的推理和生成能力。大模型还展现出了令人印象深刻的“涌现能力”，在处理某些复杂任务时，能够表现出超出训练数据中显式学习到的能力。%例如，当模型达到一定规模时，它可能展现出推理、总结、理解隐含意图等高级语言能力，这些能力并非在训练数据中明确标注过，而是模型通过大规模学习自然语言数据所涌现出来的。
然而庞大的规模也为大语言模型的部署和使用带来了诸多挑战。首先，模型的巨大参数量和复杂的计算需求使其依赖昂贵的计算资源和存储设备。在训练和推理阶段，大量的存储和计算能力是不可或缺的，这使得大语言模型的部署成本非常高。无论是在科研领域还是商业应用中，为了支持如此巨大的计算负载，通常需要投入大量的资金用于购买高性能的硬件设备，如图形处理单元（GPU）或张量处理单元（TPU）。此外，由于模型的庞大体积，延迟和实时性也成为了实际应用中模型推理时的难题。例如，部署大语言模型进行实时对话或在线服务时，模型的推理速度和响应时间往往会受到计算资源的限制，可能导致用户体验不佳。%尤其是在需要多个并发请求时，计算资源的不足可能导致服务器负载过高，影响系统的稳定性和响应效率。
为了提高模型的推理效率，基于其以Transformer核心架构的特性和自回归的生成模式，大部分大模型都设计了KV缓存避免重复计算，从而减少计算开销，优化推理过程。

大模型的核心架构Transformer通过多头自注意力(Multi-head self-attention)\cite{vaswani2017attention}机制建模序列中任意位置之间的依赖关系。自注意力机制根据“查询”（Query）和“键”（Key）之间相似度（内积）来决定每个词元对其他词元的关注程度，并使用“值”（Value）来更新每个词元的表示，Query、Key和Value都由输入向量通过模型参数映射得到。每个头使用独立的投影参数矩阵，用来捕捉不同的语义和句法关系，提高了表达容量，并且多头结构也提高了模型的鲁棒性。传统的Transformer最初采用编码器—解码器(Encoder-Decoder)双路结构，编码器负责编码输入语义，解码器再依据上下文逐步生成输出；而当模型规模扩展至超大参数量、面向按时间顺序逐个生成词元(token)的自回归生成这一核心任务时，模型的结构逐渐转向仅解码器 (decoder-only) 架构。编码器在单次前向推理时为整个序列中参与注意力计算的所有词元生成对应的Query/Key/Value，不需要跨时间步复用历史的KV。%和编码器—解码器架构(如T5\cite{raffel2020exploring})，模型通常在推理时一次性处理整个输入序列，不需要逐个词元自回归生成，因此每次推理都需要计算所有词元之间的注意力，也就需要计算所有词元的Query，Key和Value。
而仅解码器架构的自回归生成模式在每生成一个新词元时都需要用到序列中此前所有词元的Key和Value来计算注意力输出，需要对历史词元的KV进行复用，为了避免每次都对这些值重新计算引起额外的开销，大模型的KV缓存能够在推理时将每个词元的Key和Value存储起来，后续新词元在自注意力层中用到它们时就不需要重新计算了，提高了模型的推理效率，如图\ref{fig:kv_cache}所示。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figures/kvcache.pdf}
    \caption{大模型KV缓存}
    \label{fig:kv_cache}
\end{figure}

大语言模型在推理阶段依赖 KV 缓存显著减少重复计算、提升生成效率，但这种将所有历史词元的 Key/Value 表征显式保留下来的做法也带来了巨大的显存负担。具体而言，模型的每一层、每一个自注意力头都需要为序列中的每个 token 维护一对 KV 向量，累积起来的缓存体积等于“模型层数 × 头数 × 序列长度 × KV 特征维度 × 存储数据类型大小”。对已经预训练完成的模型而言，层数、头数和特征维度在推理时通常固定不变，因此整体缓存规模几乎完全受序列长度驱动，并以线性方式增长。当输入长度达到上万 token 级别时，即便在单批次推理下，KV 缓存也很容易消耗几十 GB 的显存，使得部署者不得不在上下文长度、批量并发和可用硬件之间做艰难权衡。更棘手的是，庞大的 KV 缓存不仅占据宝贵显存，还持续消耗内存带宽：每生成一步，模型都需要对缓存进行随机访问和写入，访存成为与算力同等重要的性能瓶颈。如果推理系统需要在多设备间传输 KV 缓存（例如流水线并行、张量并行或预填充和解码分离的场景），跨设备的通信延迟与带宽限制又会成倍放大该成本。由此，KV 缓存随序列长度增长所引发的存储压力和访存开销，已经演化为制约大语言模型推理可扩展性、尤其是长上下文场景下的核心问题之一。

\section{本文研究内容和研究贡献}
\subsection{研究内容}
尽管KV缓存显著降低了大模型自回归推理中的重复计算，但其随序列长度线性增长的存储需求，仍然使长上下文推理面临显存压力、带宽瓶颈等问题。如何在保持模型精度的同时压缩KV缓存的规模以节省显存和减少访存开销，已成为优化大模型推理的重要方向。

KV缓存的规模不仅与输入的序列长度有关，也由模型层数，注意力头数，每个词元的KV向量的特征维度大小和其存储在显存中的数据类型决定。因此在考虑大模型KV缓存压缩的时候，可以从上述几个不同的角度入手。\textcolor{red}{}从序列长度即模型要处理的词元数量的角度，可以使用滑动窗口只保留最近的词元，或丢弃，合并一些不重要的词元来压缩序列长度；%但是这可能影响原本序列的上下文完整性，永久地损失掉一些对后续生成过程有用的信息；
从模型不同层的角度，可以通过层间共享KV等方式进行KV缓存的丢弃和复用，来实现压缩；从同一层的不同注意力头的角度，可以通过合并一部分注意力头来减少头维度的KV总数，但一般涉及模型架构的修改，需要后训练或预训练；从数据类型的角度，可以通过量化的方式将KV缓存从高精度数据类型转为低精度数据类型，直接压缩每个元素的字节数。%然而不同层承担的语义职责差异明显，不同注意力头也会有对不同词元的不同关注程度，因此层级或注意力头级的压缩一般需要进行后训练或将模型从头预训练，否则会使得模型效果下降较多。}

\textcolor{red}{}本文主要聚焦的是从特征维度对KV缓存进行压缩这条技术路线。已有研究\cite{yuan2023asvd,chang2025palu}通过实验观察发现，大模型的KV矩阵（词元数量乘以每个词元的KV向量构成的矩阵）的特征维度存在冗余。如图~\ref{fig:energy_key_0}和~\ref{fig:energy_value_0}所示，KV的奇异值能量（信息）大部分集中在少数几个主成分上，尾部的奇异值能量占比小，存在冗余信息，因此可以根据低秩性对特征维度进行压缩。比如使用奇异值分解SVD（Singular Value Decomposition）或主成分分析PCA（Principal Component Analysis）对原本的KV进行降维投影。词元的 KV 缓存本质上由词元隐状态与模型中对应的 KV 投影矩阵相乘得到；因此，如果能够在保持主要语义信息的情况下压缩这一投影矩阵的列空间，就能直接实现特征维度的降维。基于奇异值分解的思路，即将原始的 KV 参数矩阵拆解成两个低维矩阵 $P$ 与 $Q$ 的乘积，只需在推理时存储词元隐状态与 投影矩阵$P$ 相乘得到的低秩表示，再通过 与重建矩阵$Q$相乘进行重建即可。%根据 Eckart–Young–Mirsky 定理，奇异值分解在给定秩约束下提供了最优的二范数近似，
本文的核心研究，就是围绕 SVD 驱动的 KV 特征降维框架展开。\textcolor{red}{}基于SVD特征降维压缩KV缓存的技术路线已经有一些方法有过探索\cite{chang2025palu,yuan2023asvd,zhang2024lorc}，但由于他们对于参数矩阵分解的方式以及在整体KV压缩率固定的情况下分配各层压缩率的方法仍不够完善，因此在特征维度压缩KV后模型精度下降较大。在模型推理过程中KV参数矩阵并非孤立存在，需要与输入进行计算，之前的方法考虑到了参数矩阵会与激活值相乘，用激活值的分布来指导参数矩阵的分解。而大模型的注意力层中，Value参数矩阵不仅要和激活值相乘，注意力计算的输出实际上是Query和Key相乘得到的注意力分数依次与激活值，参数矩阵相乘得到的。在SVD时引入注意力分数的影响，直接优化注意力输出在低秩近似前后的误差可以进一步减少压缩后模型的精度损失。因此，本文的一个主要研究内容是如何在做基于SVD的KV缓存特征维度压缩时考虑到激活值和注意力分数的影响。此外，此前的方法考虑到模型不同层在推理时重要程度不同，因此设计了层间秩分配（即压缩率分配）方案。但这些方案在考虑每层的重要程度时（敏感性或误差累积等角度），其他层被看作处于未被压缩的状态，模型的整体压缩率也与目标压缩率相差很大，而每一个其他层的压缩状态会对目标层的重要程度产生影响。并且之前的部分方法在考虑依据层重要程度的一个指标分配秩时还可能会导致对另一个层重要程度指标起到相反作用。(如LORC\cite{zhang2024lorc})仅考虑了基于误差累积的秩分配，但该工作认为压缩引起误差累积小的那些层中反而有一部分对压缩更敏感。）因此，在当前满足模型整体压缩预算的秩分配下，如何在所有层都处于压缩状态时根据每层KV的重要程度来动态分配压缩率，成为本文的另一项核心研究内容。

% preamble:
% \usepackage{graphicx}
% \usepackage{subcaption}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/singular_values_energy_31_key.png}
    \caption{Key累积奇异值能量}
    \label{fig:energy_key_0}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/singular_values_energy_0_value.png}
    \caption{Value累积奇异值能量}
    \label{fig:energy_value_0}
  \end{subfigure}
  \caption{Llama3.1-8B-Instruct第0层的Key和Value累积奇异值能量}
  \label{fig:two_plots}
\end{figure}
\textcolor{red}{}此外，从图~\ref{fig:energy_key_0}和~\ref{fig:energy_value_0}中的奇异值累积能量占比可以看出，尽管Key和Value都表现出了低秩性，即奇异值能量占比都快速上升，少部分的头部秩就到达了80\%以上的较高累积能量占比，但Value和Key的奇异值能量分布并不完全一样：Key的尾部奇异值几乎不提供能量，而Value的尾部奇异值虽然单个能量占比小，但其持续累积仍然使得奇异值能量占比不断上升。这表明，Value矩阵中幅值较小的奇异值确实包含冗余信息，但并非毫无价值，它们对总信息量仍有一定贡献。之前基于SVD的特征维压缩方法对于Key和Value的尾部奇异值都是直接丢弃，这对Key来说不会有明显的信息损失，然而对于Value，每个奇异值“非1即0”的硬截断丢弃方式会造成冗余信息和仍有用的一部分信息一同被去除，尤其是高压缩率场景下，会导致Value缓存的信息严重损失，模型精度显著下降。为了达到压缩Value缓存的目的，Value的尾部奇异值相较于信息量更多的头部奇异值需要更高的压缩率，但彻底丢弃又会造成信息损失。因此本研究进一步探索的研究内容是如何在保持压缩率不变的情况下，既保留头部奇异值的细节，又能以“分辨率”保留尾部秩而非将其丢弃。

%这样结合量化就可以不出现在研究内容里面，研究内容的描述还是围绕低秩特征降维展开的，而量化则是实现第三个研究目标的方法。

%因此直接对参数矩阵做SVD得到的并不是相乘结果的最优低秩近似。为了解决这个问题，之前一些方法考虑到了激活值（参数矩阵输入）的分布，用激活值来指导参数矩阵的分解，使得两者相乘结果在低秩近似前后误差尽可能小。而在大模型的注意力层中，Value参数矩阵不仅要和激活值相乘，注意力计算的输出实际上是Query和Key相乘得到的注意力分数与依次激活值，参数矩阵相乘得到的。在SVD时引入注意力分数的影响，直接优化注意力输出在低秩近似前后的误差可以进一步减少压缩后模型的精度损失。因此，本文的一个主要研究内容是如何在做基于SVD的KV缓存特征维度压缩时考虑到激活值和注意力分数的影响。

%它要与激活值、注意力分数等多个张量反复交互。如果直接对整块参数做静态 SVD 并替换原投影，会在激活值和注意力的数值尺度上引入显著偏差，导致模型精度急剧下降。如何解决这一问题，是本文的核心研究内容。

%\textcolor{red}{在推进 KV 缓存特征维度压缩时，模型总体KV缓存压缩率固定的情况下，如何分配每一层KV压缩率会对最终模型的精度下降有较大影响。由于每一层KV参数矩阵本身的低秩性、对压缩的敏感性有差异，以及由于其在模型中所处位置不同导致压缩该层带来的误差累积不同，需要为每层的KV分配不同压缩率。现有的方法考虑到了这些层的性质，从每层对KV压缩的敏感性或从减少误差累积的角度出发，设计了层间的秩分配方案。但这些方案在考虑每层的重要程度时（敏感性或误差累积等角度），其他层被看作处于未被压缩的状态，模型的整体压缩率也与目标压缩率相差很大。而每一个其他层是否压缩都会对目标层的重要程度产生影响，之前的秩分配方法由于忽略了这个因素使其效果仍然有提升空间。而且之前的部分压缩方式在考虑依据层重要程度的一个指标分配秩时还可能会导致分配的策略对另一个层重要程度指标起到相反作用。(如LORC\cite{zhang2024lorc})仅考虑了基于误差累积的秩分配，但他认为误差累积小的层反而有一部分对压缩比较敏感。）因此，在当前满足模型整体压缩预算的秩分配下，如何在所有层都处于压缩状态时根据每层KV的压缩敏感性和压缩导致的误差累积以及低秩性来动态分配压缩率，成为本文的另一项核心研究内容。}

%在推进 KV 缓存特征维度压缩时，必须同步考虑大模型各层的职能差异。普遍的认识是：浅层多承担基础词法/句法特征抽取与常识记忆，深层则主导语义理解、跨句关联以及复杂推理。正因如此，各层对维度压缩的敏感性截然不同；浅层的压缩误差还会沿网络层层放大，最终影响深层表示。若仍以统一的压缩率“一刀切”，往往要么对关键层压缩过度、牺牲性能，要么整体压缩率受限、难以显著节省显存。因此，在既定的总体压缩预算下，如何根据层的功能需求、层敏感性或误差累积等指标，动态分配各层的特征维压缩比例，成为本文的另一项核心研究内容。

%\textcolor{red}{基于SVD的KV特征降维自身也存在一定不足：%（由于存储的压缩后KV中间值需要与重建矩阵Q相乘会引入额外计算，导致该条技术路线计算开销较大；）
%由于KV缓存的低秩性有一定限度，在追求极致压缩率的场景下仅使用SVD特征降维单一方案压缩模型效果会严重劣化。为了在能够充分利用KV低秩性的同时进一步保持低压缩率下模型的精度，需要将特征维度压缩与其他技术路线结合。量化压缩KV缓存兼容性好，从数据类型的角度出发，直接压缩KV向量每个元素的字节数，易于和其他压缩KV的技术路线正交结合。之前的基于SVD的特征压缩方法\cite{chang2025palu}考虑将量化引入时忽略了低秩压缩自身的特点，只是简单地用均匀量化将所有奇异值对应的特征维度压缩到同一数据类型，在原本低秩的基础上量化后模型精度会有进一步下降。因此本研究进一步探索的研究目标是在低秩压缩的基础上，如何根据KV的低秩特点对缓存量化压缩，使模型性能不劣化较多的同时追求更高压缩率。}


%由于KV本身的低秩性质有限，当压缩率很高时，继续丢弃秩来减少特征维度实现压缩会使得模型精度大幅下降，尤其是对于低秩性本就稍差的Value，因此之前的方法都没有通过低秩特征维压缩将KV缓存压缩到很高的压缩率。此外，基于SVD的低秩特征维压缩属于"以算换存"的思想，在减少了KV体积的同时引入了推理时对低维KV重建的额外计算开销，适合应用于访存瓶颈而非计算瓶颈的场景下。之前的基于低秩分解压缩KV的方法并没有对KV的重建开销进行优化，不可避免的使压缩后模型推理过程变慢。为了解决上述的两个问题，本研究考虑将量化的技术路线结合到KV低秩压缩中来。现有的低秩压缩方法中，\cite{chang2025palu}也为了更高的压缩率也考虑了结合量化方案，但只是简单地将特征维度压缩之后的KV缓存用统一的精度进行量化，而忽略了低秩压缩自身的特点，并且并没有利用量化低精度数据类型的特性对重建开销进行优化。因此本研究进一步探索的研究目标是在低秩压缩的基础上，如何根据KV的低秩特点对缓存进一步量化压缩，使模型性能不劣化较多的同时得到更高压缩率；以及如何利用量化低精度数据类型的特点优化推理时KV缓存重建的速度。}

%在这种情况下，将量化压缩数据类型与低秩特征维度压缩结合，可以在相同的低压缩率下实现比仅低秩降维压缩更低的模型精度损失；类似地，当压缩率很低时，继续使用更低精度的数据类型进行量化效果也会差于量化与低秩压缩特征维结合的方式，因此当压缩率较高时结合低秩和量化联合压缩对减少模型精度下降是有必要的。现有的将低秩压缩和量化结合\cite{Palu}的方式只是简单地将将特征维度压缩之后的KV缓存用统一的精度进行量化，而忽略了低秩压缩自身的特点。除此之外，量化得到的低精度数据类型的乘法计算也可以对基于SVD的低秩压缩存储的降维中间值的重建进行加速。因此本文进一步探索的研究目标是在低秩压缩的基础上，如何根据不同秩包含信息量不同的特点对KV缓存进一步量化压缩，以得到更少的精度损失和推理时更快的KV缓存重建过程。

%之前的方法(SALS\cite{})，采用对Value量化而Key低秩压缩的KV缓存压缩方案，但当要求对Value的压缩率较高时，仅使用量化一种方式会导致模型性能下降较大。

%之前先低秩再压缩的方法存在的问题以及之前对V只压缩存在的问题（这俩也可以作为baseline对比）：1.只压缩V，用固定数值类型的问题：}

%在面向实际部署的场景中，使用单一角度对KV缓存进行压缩往往存在“压缩比-模型精度”的硬折衷：例如只做特征维度压缩，当压缩比超过某个阈值后，困惑度与下游任务表现会快速劣化。基于此，本文提出进一步研究“KV特征维度压缩 + 其他角度压缩手段”的复合策略，并以压缩KV缓存数据类型，即量化方式为代表进行展开研究。其动机在于：在给定的总体压缩率目标下，将压缩负担分摊到不同维度（特征、数据类型等），可以让每一维都处于相对安全的压缩区间，避免任何单一手段被迫“压到极限”导致性能断崖式下降。同时，特征维压缩聚焦于降低矩阵维度、量化则缩减位宽，二者在数值误差和实现成本上互补，可通过联合优化或分阶段校准最大化保真度。在量化方法中，更细的粒度往往有更好的效果。比如对KV缓存中每一个词元的向量分别量化效果会由于对整个KV缓存矩阵量化。但由于压缩后每层的KV缓存特征维度的不固定，直接使用细粒度量化转换KV缓存数据类型存在困难。因此，本文进一步探索与特征维度自适应压缩协同的分组细粒度量化方案，探索多策略协同压缩的机制与效果，使模型在既定显存预算下保持精度，提升长上下文推理能力。

\subsection{研究贡献}
通过对SVD驱动的KV缓存低秩近似特征降维方式、层间秩分配算法和对Value缓存的头尾部奇异值采取不同“分辨率”存储的KV缓存压缩策略的研究，本文主要做出了以下贡献：
\begin{itemize}

\item \textcolor{red}{}本研究在KV缓存SVD特征维压缩的框架下，提出了创新的分解算法和层间秩分配算法。为了解决现有工作仅引入激活值指导SVD导致模型精度下降仍然较多的问题，我们建模了注意力分数对注意力输出的低秩近似误差的影响，构建“注意力-激活值–参数乘积”在低秩近似前后的最优重构问题来优化注意力输出的近似误差，相比之前基于SVD的KV特征压缩方法能够进一步减少压缩后模型的精度损失。在求解优化问题的过程中，考虑到了MHA不同注意力头（或GQA注意力组）存在的不同注意力分布，保留原本KV尽可能多的信息，使每个注意力头（或注意力组）的输出在分解重建前后误差尽可能小；在进行不同层的秩分配时，之前的方法在模型其他层保持完整的状态下考虑目标层的重要程度，会对模型压缩状态下层重要性产生误判。本文根据每层KV的低秩情况对模型在目标KV整体压缩率下进行秩分配的初始化；考虑到了其他层压缩对目标层的影响，在每一层都处于当前秩分配的压缩状态下，根据模型输出的变化端到端地衡量每一层KV压缩导致的误差和后续误差累积，并结合输出误差对秩变化的敏感性动态迭代更新每层的秩分配情况，取得了相较于此前秩分配方案更低的压缩后模型精度下降。

%\item \textcolor{red}{在对模型不同层分配KV缓存压缩率的问题上，之前的方法在模型其他层保持完整的状态下考虑目标层的重要程度来对其分配秩。但任何一层的压缩情况都会对目标层产生影响，从而使其压缩敏感性，压缩带来的误差累积等性质发生变化；并且此前的方法对如何衡量每层KV的重要程度设计不够全面。本文根据每层KV的低秩情况对模型在目标整体压缩率下进行秩分配的初始化；考虑到了其他层压缩对目标层的影响，在每一层都处于当前秩分配情况的压缩状态下，根据模型输出的变化端到端地衡量每一层KV对压缩的敏感性和压缩其带来的误差累积，并结合输出误差对秩变化的敏感性动态迭代更新每层的秩分配情况，还设计了基于动态规划的算法加速迭代过程。相比之前的策略，本文的层间秩分配算法能够在固定的显存预算下更大程度减少模型效果的劣化。}

\item \textcolor{red}{}Value相较于Key在低秩性上存在差异，Value尾部的奇异值仍然对总信息量有一定贡献。现有基于SVD的特征压缩方法“非1即0”地直接丢弃尾部奇异值会造成Value的信息损失，压缩后模型精度下降大。为此我们提出了一种低秩感知的量化压缩方案：不再将 Value 的尾部奇异值完全丢弃，而是对其进行低精度量化。以“低分辨率”的形式保留尾部奇异值；同时对信息量更集中的头部奇异值使用更高精度（或全精度，取决于压缩率）维持细节，通过保留Value更多的信息缓解了模型能力退化。%为此我们提出用低精度量化压缩Value的尾部奇异值而非将其完全丢弃。对于头部的奇异值保留其细节，对于尾部的奇异值则以量化后的“低分辨率”存储，在保留更多信息的同时实现压缩的目的。
%低秩感知的量化压缩不仅通过保留Value尾部奇异值中的有用信息缓解了模型能力退化，
更进一步，低秩感知的量化压缩还为如何将特征压缩与量化压缩两条技术路线结合提供了更好的框架。之前也有方法\cite{chang2025palu}尝试将特征维度压缩与量化压缩结合来提高模型压缩率，但只是在特征降维后的KV缓存上直接施加量化，模型精度损失仍然较大。相比简单的"低秩降维后量化"，我们的低秩感知的量化方案对Value头部奇异值使用更高精度来保留更多信息，对尾部奇异值使用较低精度来避免完全丢弃带来损失，在同等压缩率下这种混合精度的量化方式使模型效果得以更好的保持。除此之外，低秩感知的量化还为提高KV缓存的压缩率后模型精度的持续下降问题提供了一种更好的解决方案。将低秩感知混合精度量化中的量化方式从简单的通道粒度量化升级为SOTA的量化方法，能够在进一步压缩头部和尾部奇异值数值精度的同时，维持模型能力相对原本的简单量化方式不下降，且相较于单独使用SOTA的低精度量化方法模型精度下降更低，在性能劣化更少的前提下带来更高的压缩率上限。%在对Value进行更高压缩率的低秩感知量化压缩时，我们对裁剪秩之后的Key缓存也进行了量化，使KV之间压缩率的比值保持不变。


%\item  \textcolor{red}{KV缓存的低秩性存在一定限度，对于特征维度压缩KV的方式，当压缩率超过一定限度时继续丢弃秩会使模型精度大幅下降，往往需要与其他技术路线结合来追求更高的压缩率。%并且由于Value更弱的低秩性，对参数矩阵分解后保留的特征维度更多，因此重建时会带来更多额外的推理时间开销。
%为了解决上述问题，本研究在提出的KV特征维度压缩算法的基础上，引入了量化来追求更高的压缩率。
%量化这一技术路线和其他维度压缩正交性好，此前已有将特征维度压缩与量化压缩结合的先例。但之前的在低秩分解之后再量化的方案只是简单地将"低秩+量化"叠加使用，对所有降维后的KV缓存使用同一数据类型均匀量化，忽略了Key和Value的低秩特点。%，而且并不能实现加速低秩KV的重建。而
%本研究考虑到奇异值大小不同的秩信息的冗余度也不同，且Value的尾部秩仍存在有用信息，因此设计了基于秩重要程度的混合精度量化方案。%并提出了一种理论上可行的低精度矩阵乘法优化Value重建开销的方案。
%使用本文提出的低秩感知的混合精度量化将我们的KV特征降维算法与量化方式结合，能够在极高的压缩率下仍然保留模型的精度，且对于state-of-the-art的量化压缩方案能够实现“即插即用”。}%，并且能够缓解低秩分解带来的Value缓存重建速度慢的问题。}

\item \textcolor{red}{}本文在实验层面进行了覆盖“基础语言建模能力--下游推理--长上下文泛化”的系统评测，用以全面验证本文提出的 KV缓存特征维压缩算法以及低秩感知的量化策略的有效性与泛化性。具体而言，我们在两类代表性指令模型（Llama3.1-8B-Instruct 与 Qwen2.5-7B-Instruct）上，分别在轻量压缩率（30\%）、中等压缩率（50\%）以及更高压缩率（75\%，87.5\%，结合量化）下，联合报告语言建模困惑度（WikiText-2\cite{merity2016pointer}、PTB\cite{marcus-etal-1993-building}）、零样本推理准确率（覆盖 BoolQ\cite{clark2019boolq}、HellaSwag\cite{zellers2019hellaswag}、ARC\cite{allenai:arc}、PIQA\cite{Bisk2020}、CEval\cite{huang2023ceval} 等多任务）与 LongBench\cite{bai2023longbench} 长上下文评测（覆盖多文档/单文档问答、摘要、Few-shot、合成检索、代码等任务类型）的结果，并与特征维度压缩，量化压缩的方法进行对比，从而刻画我们在不同任务形态与不同预算下的精度-存储情况。我们提出的低秩感知混合精度量化提供了一种特征维和量化压缩的结合方式，为了公平地与量化方案进行对比，我们实现了SOTA的KIVI\cite{liu2024kivi}量化压缩KV的低秩感知量化版本来与量化技术路线的方法对比。进一步地，我们设计了面向关键模块的消融实验：对比之前SOTA方法与本研究在 SVD 分解方式与层间秩分配方式上的交叉组合，探究各模块对模型能力的贡献来源。%并对是否量化重建矩阵$Q$进行消融，验证我们提出的理论利用低精度矩阵乘加速Value缓存的方式能够在几乎不牺牲精度前提下提供了更高推理效率的可能。
上述系统实验证明了本文方法在多模型、多任务、多压缩率下均具备稳健优势。

%\item 本研究在KV缓存SVD低秩分解特征维压缩的框架上，显式建模激活值与模型内不同注意力头（注意力组）的注意力分数对参数矩阵的影响，构建“激活/注意力–参数乘积”在低秩近似前后的最优重构问题，推导出一套创新的先对参数矩阵进行预变换再分解，最后在重建时通过逆变换还原的步骤，使压缩后的KV缓存既保留原矩阵的主导子空间，保留了原本KV尽可能多的信息，也保证了模型推理时参数矩阵与激活值和不同注意力组（注意力头）的注意力相乘的计算结果误差在分解重建前后尽可能小。


%\item 本研究利用KV缓存层间低秩性质的差异，结合不同层之间压缩导致误差在后续模型层中传播的误差累积，以及对秩减少或增加的敏感性，提出了创新的层间压缩预算分配策略。该策略以整体压缩率为约束，根据不同层KV的低秩性分配每层的初始压缩率，并根据每层压缩带来的误差累积大小，以及每层的KV对秩增加和减少的敏感程度，动态调节分配给每层的秩，从而在固定的显存预算下更大程度减少模型效果的劣化。

%\item 本研究针对压缩后各层的KV缓存维度不固定，难以引入细粒度量化与特征维度降维结合的问题，设计了可变化的混合高低精度和粗细细粒度的量化流程，对于Key和Value根据其不同的低秩性质设计了不同的"低秩-量化"结合的策略。对于低秩性更好的Key采用先低秩再量化；对于低秩性较差的Value，将低精度细粒度量化目标绑定到SVD投影后的奇异值较小的部分，高精度粗粒度量化目标绑定到奇异值较大的部分。我们的策略结合了低秩和量化的优点，既避免单一手段被迫过度压缩，又让总压缩率在多维组合下分摊误差，最终实现模型推理时相同压缩率下更少的精度下降。




\end{itemize}

\section{本文结构介绍}
第一章绪论首先从研究背景，研究内容和研究贡献的角度介绍了KV缓存在大模型推理时的作用和在长文本推理时其带来的显存占用和访存开销的问题，压缩KV缓存的重要意义以及本研究所提出的低秩特征维度压缩算法和低秩感知的混合精度量化策略的主要贡献，创新性以及解决了该研究背景下的仍存在的问题。第二章进一步介绍了当前国内外对KV缓存压缩问题的研究情况。第三章介绍了本文的KV压缩方法即主要贡献：以SVD分解为框架，建模注意力分数，激活值对参数矩阵的作用并求解优化问题使得压缩前后注意力输出差异最小；考虑每一层压缩情况对彼此的影响，根据压缩带来的误差和误差累积在固定KV缓存预算下动态分配每层的压缩率；考虑到Value尾部奇异值仍包含有用信息，在保持压缩率不变的前提下将其对应特征维度量化为低精度存储，替代原本直接截断丢弃的压缩方式。在需要极高压缩率的场景下，进一步对头部和尾部奇异值使用混合精度量化压缩Value缓存，使总压缩率在多维组合下分摊误差，保证模型精度不会有很大损失。第四章详细地给出了实验中我们压缩方法的优秀表现，并和之前的KV缓存特征维度压缩方法，量化压缩方法进行了对比。在不同压缩率的设定下，分别对不同的开源大模型执行注意力-激活感知的特征维度压缩以及与低秩感知量化压缩的策略，在语言建模困惑度评估、零样本推理下游任务以及长文本基准上给出了全面测试结果，以此证明方法的稳定性和泛化能力。最后第五章则对全文内容进行了总结并提出了下一步研究展望。