\chapter{低秩感知的混合精度多粒度量化KV压缩策略}
\label{chap:quant_low_rank}
~\ref{chap:scaling_svd}章节和~\ref{chap:rank_search}章节已经给出了我们完整的低秩分解压缩KV缓存以及层间分配压缩率的算法。但由于分解目标本身的低秩性也是有限的，尤其是对Value来说，若希望进一步压缩，需要与其他维度的KV缓存压缩方式结合，比如引入低比特量化。然而已有实验表明：当 Key/Value 被粗粒度（比如每个词元使用同一组量化参数）地统一压到低精度2-bit 时，模型困惑度急剧上升。而当我们提高2-bit量化的粒度，将通道粒度量化改为通道内分组粒度量化时，模型压缩后的效果会有不小的提升。表~\ref{tab:value-quant-granularity}展示了我们上面的结论，其使用的数据集WikiText-2由 Wikipedia 精选的文章段落构成，PTB为《华尔街日报》语料的精简版本，后续实验章节我们会详细介绍它们。衡量压缩后模型性能的指标为困惑度（ppl），越大说明模型性能越差。
\begin{table}[htbp]
    \centering
    \caption{不同量化粒度在 2-bit 下的困惑度（Perplexity）对比}
    \label{tab:value-quant-granularity}
    \begin{tabular}{lcc}
        \toprule
        \textbf{量化粒度} & \textbf{WikiText-2} & \textbf{PTB} \\
        \midrule
        通道级（channel-wise, 2-bit） & 514.59 & 2665.66 \\
        分组级（group-wise, 2-bit） & 11.17 & 20.87 \\
        \bottomrule
    \end{tabular}
\end{table}

在低秩压缩后，一旦想继续将量化应用到低秩压缩的模型上，由于层与层之间保留的秩彼此不同，量化时KV缓存的特征维度就都不相同， group-wise 量化的组大小往往无法整除各层KV缓存的特征维度，导致细粒度量化难以落地。最后，即使解决了粒度问题，我们仍需在不同秩之间区别对待量化精度：低秩分解所暴露的奇异值能量差异意味着重要秩必须维持较高精度，因为其本身包含了模型的更多信息，而长尾秩（比如原本低秩压缩舍弃的部分）则可采用更低比特。如何在这三重约束下，把“粗/细粒度量化”与“混合精度、低秩感知”协同起来，构成了本章方法的出发点。

\section{背景知识}
在正式介绍我们的方法之前，为了能更清楚地描述我们是如何将量化与低秩压缩结合的，我们先对量化这一数据类型角度压缩KV缓存的概念，量化时常见的一些问题和常用的解决方案进行介绍。
\subsection{量化基础}
KV缓存一般以半精度浮点数的形式（FP16）存储在显存中。将其从16比特压缩到 $b$ 比特整数表示时，我们通常对每个量化单元（可对应所有词元的KV缓存、部分词元的KV缓存、单个通道（一个词元的KV缓存向量）或一个通道内的部分特征即一个组）估计一个放缩系数 $s$ 与可选的零点 $z$，使得
\begin{equation}
    q = \operatorname{clip}\!\left(\left\lfloor \frac{x}{s} + z \right\rceil, q_{\min}, q_{\max}\right), \qquad
    \hat{x} = s \cdot (q - z),
\end{equation}

其中 $x$ 与 $\hat{x}$ 分别表示原始值与反量化之后的值，$q$ 为量化后的整数值。若 $z=0$ 并强制 $q_{\min}=-q_{\max}$，即得到\textbf{对称量化}；其实现简单、硬件友好，但需要数据分布在零点附近且正负范围大致相当。相比之下，\textbf{非对称量化}允许 $z\neq 0$，通常以数据的最小值对齐零点，从而更好地覆盖偏移分布。

然而，无论采用哪种策略，离群值（outlier）都会显著放大量化误差。若采用单一尺度 $s$ 覆盖所有值，少量幅值巨大的 KV 元素会迫使尺度增大，从而让绝大多数常规值落在更粗的量化间隔内，最终抬高模型困惑度；而离群值中本身可能包含较重要的信息，若强行截断离群值，也会造成信息的损失。下面举一个例子来说明离群值的影响：比如量化目标为8-bit的INT8数据类型，在没有离群值的情况下向量[0.68,  0.75,  0.81,  0.94]根据计算会量化到[91,  101,  109,  127] ；如果最后一个值是离群值，比如[0.68,  0.77,  0.81,  9.72]就会被量化到[8,  10,  10,  127]，可以看到离群值的存在让本身差异较大的前三个元素在量化后变得几乎没有差异了。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/outlier.png}
    \caption{KV缓存离群值分布}
    \label{fig:outliers}
\end{figure}

图~\ref{fig:outliers} 展示了我们在实际 KV 缓存中观测到的离群值现象。图中以Key缓存为例，Hidden这一轴表示低秩压缩后Key缓存的特征维度，Seqlen这一轴表示词元的序列维度。从特征维度的角度看，少部分特征维度的绝对值远高于其余特征维度；从序列维度的角度看（即逐个词元），同一词元的特征维度存在明显的离群值现象，这也是粗粒度（词元粒度）量化效果退化严重的原因之一。通过结合细粒度划分与混合精度策略能够一定程度上抑制离群带来的损失。除此之外，现有的一些方法也提出了对量化目标矩阵进行矩阵变换消除离群值之后再进行量化，并对量化后的矩阵进行反变换的方式来消除离群值影响，这种方法同样可以应用到我们的场景下。

\subsection{Hadamard 变换与离群抑制}
为缓解离群幅值导致的动态范围膨胀，上述提到的矩阵变换中一类常见手段是在量化前施加 Hadamard 变换，最早在\cite{}中被用来消除量化模型参数时的离群值。Hadamard 矩阵 $H_n\in\{\pm 1\}^{n\times n}$ 是一族正交矩阵，满足 $H_n H_n^\top = n I$，其变换可通过快速 Walsh–Hadamard 变换（FWHT）在 $O(n\log n)$ 时间内完成，不涉及乘法，仅需加减法与轻量的缩放。我们对单个量化单元的向量 $x\in\mathbb{R}^n$ 施加标准化后的变换
\begin{equation}
  \tilde{x} = \frac{1}{\sqrt{n}} H_n x,  
\end{equation}
然后在 $\tilde{x}$ 空间执行量化。由于 Hadamard 变换实质上把每一维度投影到 $\pm 1$ 组合的正交基上，原本集中于少数维度的能量会被“打散”至各通道，极大地降低单个维度的峰度（kurtosis），从而使可用的量化尺度更贴近大多数数值。反量化时只需要在反量化后施加同样的 Hadamard（其自身就是逆变换），即可还原到原始基底。

典型的 Hadamard 矩阵阶数为 $2^k$。例如，
\begin{equation}
    H_1 = [1],\qquad
H_2 = \begin{bmatrix}
1 & 1 \\
1 & -1
\end{bmatrix},\qquad
H_4 = \begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & -1 & 1 & -1 \\
1 & 1 & -1 & -1 \\
1 & -1 & -1 & 1
\end{bmatrix},
\end{equation}
更高阶矩阵可递归构造 $H_{2n} = \begin{bmatrix} H_n & H_n \\ H_n & -H_n \end{bmatrix}$。这些 $\pm1$ 结构使得 FWHT 仅需加减法即可完成正交变换，实现起来也比较高效。

尽管 Hadamard 变换无法完全消除异常值，它显著降低了极端值对量化尺度的牵制，对量化目标执行 FWHT，可在可接受的算力成本下换取更平滑的值分布。虽然 Hadamard 变换会带来额外的访存与延迟开销，但我们可以将Hadamard矩阵融合到静态的模型参数中来消除额外开销，这些实现细节将在后续方法部分讨论。


\section{低秩感知的混合精度量化}
从图~\ref{fig:energy_key}和图~\ref{fig:energy_value}中可以观察到，虽然前文提出的算法中的分解目标$S_kW_k$和$\bigl[S_{v,1}W_{v,1},...S_{v,G}W_{v,G}\bigr]$都具有低秩性，即大部分能量都只集中在前几个奇异值上，但是明显能够发现Key的低秩性要更好一些，长尾的奇异值几乎不占多少能量了。因此我们认为对于完全丢弃能量占比最小的秩的做法，对于Key来说损失的信息是可以接受的，而对于Value来说可能造成一些仍然比较有用的信息被丢弃掉了。因此在我们已经通过低秩降维的方式对模型KV缓存进行了压缩后，为了进一步压缩模型，我们将特征维度与量化结合时对Key和Value采用了不同的方法。

对于Key缓存的压缩，我们保留使用~\ref{chap:scaling_svd}和~\ref{chap:rank_search}中秩分配算法得到的低秩裁剪结果。若原始维度为$d$，压缩后每层保留$r_{k,l}$个秩，其空间占用约为原始的一部分，定义低秩压缩率为$\rho_1 = r_{k,l}/d$（或等价的存储体积比）。在该基础上，我们先以16-bit精度存储压缩后的中间值，以保证Key主奇异向量的能量基本无损；随后进一步将这部分数据量化到更低的$b$比特，例如8-bit或4-bit，使量化压缩率达到$\rho_2 = b/16$。最终Key侧的每一层的压缩率可近似写成$\rho_1 \cdot \rho_2$：其中$\rho_1$来自低秩裁剪带来的特征维降维，$\rho_2$来自位宽压缩。若对所有层求平均，则得到模型级别Key缓存的整体压缩率：
\begin{equation}
{\rho}_{\text{key}}=\frac{1}{L}\sum_{l=1}^{L} \rho_{1,l}\rho_{2,l}    
\end{equation}
由于Key的尾部能量极低的奇异值，直接将其舍弃并不会对模型效果有很大影响，因此我们只对原本低秩压缩保留的奇异值和奇异向量计算的Key缓存进行了量化，其他奇异值对应的特征维度则直接丢弃。

在量化前进一步抑制Key缓存中的离群值时，我们借鉴 Palu~\cite{palu} 中的思路，将 Hadamard 变换直接融入到线性投影中。设原始投影矩阵$S_k^{-1}S_kW_k$按低秩分解写作$S_k^{-1}S_kW_k = (S_k^{-1}P_k)Q_k$，我们存储的参数矩阵由原本的$W_k$变为了$S_k^{-1}P_k$和$Q_k$，其中$S_k^{-1}P_k\in\mathbb{R}^{d_{\text{in}}\times r}$、$Q_k\in\mathbb{R}^{r\times d_{\text{out}}}$，推理时缓存的是中间值$XS_k^{-1}P_k$。我们将Hadamard矩阵$H$吸收到前半部分，使得新的前向为$X (S_k^{-1}P_k)' = X (S_k^{-1}P_k H)$；相应地，把$H^{-1}$（在Hadamard情形下等同于$H^\top / r$）折叠进后半部分$Q_k' = H^{-1} Q_k$。如此一来，在线计算得到的中间值$X (S_k^{-1}P_k)'$在产生时就已经过Hadamard变换，天然具备离群值较少的特性，因而可以直接进行量化；反量化之后无需额外施加逆变换，只需与预处理过的$Q_k'$相乘即可还原原始投影效果：
\begin{equation}
    W_k \approx (S_k^{-1}P_k)Q_k = (S_k^{-1}P_k H)(H^\top Q_k) = (S_k^{-1}P_k)'(Q_k)'
\end{equation}
这种“线性层融合变换”的做法不仅省去了在线FWHT的额外成本，也确保了Key缓存量化后仍能保持与未量化版本一致的语义表达，与 Palu~\cite{palu} 的实现保持一致。

我们结合低秩压缩与量化的多角度压缩方式的创新点主要在Value的混合精度压缩上。前文提到，虽然对于Value缓存长尾的奇异值能量较少，提供的信息少，但相比Key缓存直接将低能量奇异值丢弃仍然可能损失一些信息。因此我们综合了量化和低秩降维的思路，采用“保留全部秩、按重要性分配不同数据位宽”的策略：对奇异值较大的重要秩维持较高精度（如8-bit），以保障生成质量；对长尾秩虽然保留，但使用更低比特（如4-bit）来进一步压缩。由于 Value 每层的保留秩 $r_{v,l}$ 不尽相同，为了保持混合精度量化后该层的Value缓存压缩率与”先低秩压缩特征维度，再量化数据类型“的压缩率一致，我们动态调整高精度量化的特征和低精度量化的特征的数量。设第$l$层通过低秩压缩的方式的压缩率为$\rho_1 = r_{v,l}/d$，再量化到更低的$b$比特，量化的压缩率为$\rho_2 = b/16$，那么该层的总压缩率为$\rho_1 \cdot \rho_2$。我们现在需要用"重要秩高精度量化，不重要秩低精度量化"的方式对$l$层达到相同的压缩率，那么就需要对高精度和低精度各自量化的秩的数量进行分配。满秩的特征维度为$d$，高精度数据类型占$b_1$比特，用于量化前$r_1$个秩，低精度数据类型占$b_2$比特，用于量化后$r2$个秩。为了保持压缩率不变，需要满足：
\begin{align}
    (b\cdot\rho_1 \cdot \rho_2 \cdot d)\text{-bit} & = (b_1 \cdot r_1 + b_2 \cdot r_2)\text{-bit}, \label{eq:equal_bit} \\
    r_1 + r_2 & = d \label{eq:rank}
\end{align}
其中~\ref{eq:equal_bit}表示保持"先低秩再量化"的压缩方式和"低秩性感知的混合精度量化"的压缩方式得到的KV缓存特征维度所占的比特位数相等，~\ref{eq:rank}表示高精度量化和低精度量化的秩数量之和等于满秩的情况。

由上述方程组可以解得
\begin{align}
    r_1 = d \cdot \frac{b \rho_1 \rho_2 - b_2}{b_1 - b_2}, \\
    r_2 = d \cdot \frac{b_1 - b \rho_1 \rho_2}{b_1 - b_2}.
\end{align}
为保证 $r_1,r_2$ 均为非负整数，我们在实现中会对结果进行四舍五入，并依据组大小或并行约束做少量调节。若 $b_1=8$、$b_2=4$，只要 $b\rho_1\rho_2 \in [b_2, b_1]$ 即可得到合理解；当某层的 $b\rho_1\rho_2$ 过小（靠近纯低比特）时，我们直接令 $r_1=0$，退化为全部低精度量化，这样做在保留全部秩的前提下仍能让压缩率与原方案对齐。

混合精度方案的优势在于：一方面通过高精度保留头部奇异值对应的方向，降低 Value 裁剪一部分秩带来的生成损失；另一方面维持与原“秩裁剪+统一量化”相当的整体体积，因而不会增加 KV 缓存预算。更重要的是，$r_1$ 与 $r_2$ 的求解完全依赖于上一节得到的秩分配和目标位宽，无需额外的调优环节；我们仅需按层读取奇异值能量排序，确定哪些秩进入高精度集合即可。后续实验章节\ref{}会展示这个方案的效果优于"秩裁剪+统一量化"。

\section{可控的混合粒度量化}
前述混合精度框架侧重于“哪些秩用高精度、哪些秩用低精度”。然而在具体实现低比特量化时，我们还必须面对粒度选择带来的性能影响：最初实验结果（表~\ref{tab:value-quant-granularity}）表明，若直接对 Value 施以通道级 2-bit 量化，困惑度会骤然升高；只有切换到更细粒度（如 group-wise）后，才能恢复到可接受水平，两者的区别在于量化目标矩阵中有多少个元素共享同一个量化的系数。这意味着，长尾秩虽然可以以低精度保存，但仍需要细粒度量化来降低量化噪声，否则就可能会导致导致模型性能的退化。

问题在于：传统“先低秩压缩再量化”的 pipeline 会导致各层保留的维度 $r_{v,l}$ 互不相同，而 group-wise 量化要求量化目标的特征数能够整除预设的 group-size（例如 32 或 64）大小，否则组内统计与反量化流程都无法对齐。这样一来，要么被迫退回到较粗粒度（牺牲质量），要么在每层为补齐分组而填充零造成额外的存储开销，都不利于 Value 缓存的低比特压缩。

我们在上一节中保留了完整的 $d$ 个秩，并且只通过高/低精度划分来匹配目标压缩率。这一设计带来额外的自由度：对于低精度集合，我们可以按组增减秩（例如增加或减少一个 group 的秩数量），再将多余的秩转移到高精度集合，从而保证低精度部分的特征数始终整除 group-size。具体做法是：先根据式~\eqref{eq:equal_bit}–\eqref{eq:rank} 求得理论的 $r_1,r_2$，随后对 $r_2$ 进行“四舍五入到最近的组数”处理，若发生偏差，则用相同数量从 $r_1$ 中增减秩以保持总数为 $d$ 并维持总 bit 数不变。因为我们仍旧持有所有秩的信息，并未真正截断尾部，所以这种秩调度只改变“哪些秩属于低精度集合”，不会破坏低秩分解本身。

借助这一“可控混合粒度”机制，我们最终实现了三重结合：重要秩使用高精度、无需细粒度；非重要秩使用低精度，并强行对齐到设定的 group-wise 粒度，如此才能真正发挥 2-bit/4-bit 量化的潜力。实验中我们发现，4比特作为低精度秩的量化目标数据类型时，group-size一般设为32就会有较好效果，而2-bit作为低精度秩的量化目标数据类型时则需要更细粒度的 group-size。因此，本章提出的方案不仅在精度维度上进行了差异化处理，还在粒度上实现了自动可控的对齐，彻底解决了“低秩裁剪导致特征数不整除 group-size”的工程瓶颈。在相同的模型预算与量化位宽下，我们的方案相较于“先低秩再量化”的传统 pipeline 能获得更低的困惑度和更稳定的推理质量。进一步地，我们也将与 Key 相同的 Hadamard 融合策略应用到 Value 的量化分支，以在保持表达一致性的同时减少在线额外变换开销。本章节提出算法的伪代码~\ref{alg:hybrid-quant}。

\begin{algorithm}[htbp]
\caption{低秩感知的混合精度与可控粒度量化流程}
\label{alg:hybrid-quant}
\KwIn{%
    校准集 $\mathcal{D}_{\text{cal}}$；\\%
    每层奇异值能量 $\{\sigma_{k,l,i}\}, \{\sigma_{v,l,i}\}$；\\%
    目标位宽 $b$、高精度位宽 $b_1$、低精度位宽 $b_2$；\\%
    目标压缩率 $\rho$、group-size $G$；\\%
    Hadamard 阶 $H_l$ 及对应矩阵 $H_l$；\\%
    低秩秩分配结果 $\{r_{k,l}, r_{v,l}\}$，来自~\ref{chap:scaling_svd},~\ref{chap:rank_search}
}
\KwOut{%
    Key 缓存量化参数 $\{s_{k,l}, z_{k,l}, Q_{k,l}\}$；\\%
    Value 缓存混合精度量化参数 $\{s^{(1)}_{v,l}, s^{(2)}_{v,l}, z^{(1)}_{v,l}, z^{(2)}_{v,l}\}$；\\%
    变换后矩阵 $\{(S_k^{-1}P_k)', (Q_k)'\}$ 与 Value 量化掩码
}
\BlankLine
\For{$l \gets 1$ \KwTo $L$}{
    \tcc{Key 分支：低秩 + Hadamard + 单一低比特}
    依据 $r_{k,l}$ 从 $S_k W_k$ 中提取主奇异向量，构造 $S_k^{-1}P_{k,l}$、$Q_{k,l}$；\\
    将 Hadamard 矩阵 $H_l$ 吸入前半部分：$(S_k^{-1}P_{k,l})' \gets S_k^{-1}P_{k,l} H_l$；\\
    将 $H_l^{-1}$ 吸入后半部分：$(Q_{k,l})' \gets H_l^{-1} Q_{k,l}$；\\
    计算低秩压缩率 $\rho_{1,l}^{(k)} = r_{k,l}/d$，位宽压缩率 $\rho_{2,l}^{(k)} = 16/b$；\\
    设定 Key 层压缩率 $\rho_{k,l} = \rho_{1,l}^{(k)} \rho_{2,l}^{(k)}$；\\
    \ForEach{$x \in \mathcal{D}_{\text{cal}}$}{
        计算中间缓存 $h_{k,l} \gets x (S_k^{-1}P_{k,l})'$；\\
        估计对称量化尺度 $s_{k,l}$、零点 $z_{k,l}$ 并量化为 $b$ 比特；\\
        累积 L2 误差以检查量化后精度，如 Section~\ref{sec:dp_for_inference} 所述；
    }
    存储 $\{s_{k,l}, z_{k,l}, (Q_{k,l})'\}$ 作为 Key 层最终参数；
    \BlankLine
    \tcc{Value 分支：混合精度 + 可控粒度}
    计算低秩压缩率 $\rho_{1,l}^{(v)} = r_{v,l}/d$，目标位宽压缩率 $\rho_{2,l}^{(v)} = b/16$；\\
    求解高/低精度秩数 $r_{1,l}, r_{2,l}$，满足式~\eqref{eq:equal_bit} 与~\eqref{eq:rank}；\\
    将 $r_{2,l}$ 四舍五入到最近的 group 数：$\tilde{r}_{2,l} = \operatorname{round}(r_{2,l} / G) \cdot G$；\\
    调整 $r_{1,l} \gets d - \tilde{r}_{2,l}$，并相应更新 $\rho_{2,l}^{(v)}$ 保持等比特约束；\\
    根据奇异值能量排序，选取前 $r_{1,l}$ 个秩为高精度集合 $\mathcal{H}_l$，其余为低精度集合 $\mathcal{L}_l$；\\
    \ForEach{$x \in \mathcal{D}_{\text{cal}}$}{
        计算 Value 缓存 $h_{v,l} \gets x S_{v,l}^{-1} P_{v,l}$；\\
        对 $\mathcal{H}_l$ 使用 $b_1$ 比特量化，得到尺度 $s^{(1)}_{v,l}$、零点 $z^{(1)}_{v,l}$；\\
        将 $\mathcal{L}_l$ 按 group-size $G$ 分组，对每组使用 $b_2$ 比特量化，记录 $s^{(2)}_{v,l,g}$、$z^{(2)}_{v,l,g}$；\\
        若分组后仍出现 outlier，则对该组单独执行 Hadamard 预处理；
    }
    保存高精度与低精度位宽掩码，并记录层级压缩率 $\rho_{v,l}$；
}
\BlankLine
输出平均压缩率 $\rho_{\text{key}} = \frac{1}{L}\sum_l \rho_{k,l}$、$\rho_{\text{value}} = \frac{1}{L}\sum_l \rho_{v,l}$ 以及全部量化参数，供推理阶段直接解码；
\end{algorithm}

\section{本章小结}
本章围绕“低秩分解 + 混合精度/粒度量化”对 KV 缓存进行再压缩展开。首先，我们分析 Key/Value 的奇异值能量差异，并提出 Key 侧采用低秩裁剪后配合 Hadamard 融合与低比特量化，Value 侧采用保留全部秩的混合精度方案。随后，针对 Value 长尾秩仍需细粒度量化的问题，我们引入可控分组机制，让低精度集合在位宽不变的前提下对齐任意 group-size，从而兼顾压缩率与精度。最后，通过统一的伪代码描述，将 Key 和 Value 的差异化策略整合进一套端到端流程。整体来看，本章方法在再前两章的基础上进一步降低了 KV 缓存体积，相比“先低秩再量化”具备更好的困惑度表现，为后续的推理部署提供了高性价比的压缩范式。