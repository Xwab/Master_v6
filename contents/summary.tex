% !TEX root = ../main.tex

\chapter{全文总结与展望}

\section{全文总结}
大语言模型在自回归推理阶段通常采用KV缓存来避免对历史词元的重复计算，从而提升生成效率。然而，KV缓存的存储规模随上下文长度线性增长，并在长文本输入、并发推理与多卡通信等场景中带来显存占用与内存带宽的严重压力，已成为制约长上下文推理可扩展性的核心瓶颈之一。围绕“在固定缓存预算下尽可能保持模型推理精度与长文本能力”这一目标，本文系统研究了基于低秩性质的KV缓存特征维压缩方法，并提出了从分解目标、层间预算分配与压缩形式三个层面协同优化的KV缓存压缩框架。

首先，在单层KV特征维压缩方面，本文从推理计算路径出发重新审视传统SVD分解的优化目标。已有低秩压缩方法考虑了激活值分布对误差放大的影响。在此基础上本文进一步指出：注意力层的Value参数矩阵不仅与激活值相乘，还会与注意力分数共同参与计算，最终影响模型的注意力输出。因此，本文提出“注意力与激活感知”的低秩分解方式：Key侧引入激活统计以对齐$XW_k$的推理目标，Value侧显式建模注意力分数对误差的加权作用，以更贴近$AXW_v$的真实推理计算；同时方法兼容GQA架构，通过对不同注意力组的统计量进行刻画来减少组间分布差异带来的近似偏差。该设计在保持SVD低秩分解可解释性的同时，更直接地优化“低秩近似压缩前后注意力输出差异”，从而减少特征维压缩带来的性能损失。

其次，在全模型层间压缩预算分配方面，本文针对“整体压缩率固定时，各层重要性评估需考虑其他层同时压缩的影响”这一关键问题提出了迭代式优化的层间秩分配策略。本文将每层（经变换后的）KV投影矩阵在SVD下由奇异向量张成的表示子空间作为分配对象，以奇异值能量与端到端输出敏感性刻画\textbf{子空间重要性}：首先利用各层分解目标的奇异值能量分布进行全局秩分配初始化，使初始分配天然反映层间低秩性差异；随后在所有层均处于压缩状态下，使用校准集端到端评估每层压缩误差及其在后续层传播的误差累积，并结合对秩变化的敏感性度量，通过迭代的秩重分配逐步减小模型压缩前后整体的输出偏差。为降低评估代价，本文还给出了动态规划式的单层解压前向过程，在无需为每层构建独立模型的情况下高效获得“单层Key/Value解压”所带来的输出变化，从而使层间秩分配算法有更优的效率。

最后，本文观察到Value在低秩性上与Key的不同：Key的奇异值能量更集中，裁剪尾部秩造成的信息损失相对可控；而Value的长尾奇异值虽然能量占比小，但仍包含一定有效信息，简单的“非1即0”裁剪容易在中高压缩率下引发明显能力退化。为此，本文提出对Value采取“头部奇异值全精度保留、尾部奇异值低精度压缩”的低秩感知量化策略代替原本丢弃尾部奇异值的方式，能够带来更大的性能收益。该策略等价于在层内\textbf{按子空间重要性分配数值精度}：重要子空间保细节，次要子空间以低精度保留而非直接丢弃。低秩感知量化策略还提供了一种在更高KV压缩率场景下结合特征维压缩和量化压缩的方式，相较于原本的“低秩裁剪后量化”以及单一的量化技术路线都能保留原模型更多的性能。此外，本文还讨论了对重建矩阵进行量化以利用低精度矩阵乘法潜在加速重建的思路，并通过实验验证其对精度影响较小，为后续工程加速提供了方向。

%最后，在更高压缩率的场景下，本文结合Key与Value在低秩性上的差异进一步提出低秩感知的混合精度量化策略。本文观察到：Key的奇异值能量更集中，裁剪尾部秩造成的信息损失相对可控；而Value的长尾奇异值虽然能量占比小，但仍包含一定有效信息，简单的“非1即0”裁剪容易在中高压缩率下引发明显能力退化。为此，本文提出对Value采取“头部秩高精度、尾部秩低精度”的混合精度量化方式，以“低分辨率”保留尾部信息，同时与特征维压缩共同满足总体预算，从而在高压缩率下缓解性能断崖式下降。此外，本文还讨论了对重建矩阵进行量化以利用低精度矩阵乘法潜在加速重建的思路，并通过实验验证其对精度影响较小，为后续工程加速提供了方向。

综合实验方面，本文在Llama3.1-8B-Instruct与Qwen2.5-7B-Instruct上，围绕语言建模困惑度、零样本推理与LongBench长上下文评测进行了系统验证，并在30\%、50\%、75\%与87.5\%等多档压缩率下与SVD特征压缩基线与量化基线进行对比。实验结果表明：在轻量压缩率（30\%）下本文方法几乎无损；在适中压缩率（50\%）下，注意力分布感知分解与\textbf{子空间重要性感知的迭代式层间秩分配}能够稳定降低困惑度退化并提升下游任务表现，使用低秩感知的量化压缩Value还能使模型精度下降进一步减少；在更高压缩率下，低秩感知的混合精度量化能够比“低秩裁剪后统一量化”以及单纯量化路线更稳健地保持模型在各任务类型和文本长度上的能力。显存测试进一步显示，在高压缩率设定下KV缓存显存占用显著降低，使模型在单卡场景下支持更长的输入上下文长度上限。总体而言，本文工作证明了：通过“更贴近推理目标的低秩分解 + 在压缩状态下端到端评估的迭代式层间秩分配 + 面向Value长尾信息的低秩感知量化”，可以在显著降低KV缓存成本的同时最大化保留大模型推理能力。

\section{未来研究展望}
面向更长上下文、更高吞吐与更强泛化的推理需求，本文工作可在以下方向继续拓展：
\begin{itemize}
  \item \textbf{面向推理内核的低开销重建：} 将低秩重建与注意力计算更深度地融合。理论上将重建矩阵$Q$直接融合到注意力层的输出参数矩阵$W_O$中可以避免重建开销，但目前版本的FlashAttention不支持完整维度的Query与特征降维后的Value和Key进行计算，因此这种先计算注意力再无开销重建的思路反而会导致更长的计算时间。因此我们进一步的研究目标之一是Flashattention算子层面的优化，未来的研究中预期对其底层实现进行重构，使其支持Query，Key和Value特征维度不对齐的高效计算。此外，本文提出的使用低精度矩阵乘法进行重建的思路需要在线进行反量化，因此未来的研究将在支持更快的低精度计算的硬件上测试其减少重建开销的效果，并且在算子层面实现低精度乘法与反量化的融合。%（例如在attention算子内部完成重建或对重建路径做算子融合/并行化），并结合更高效的低精度GEMM内核与张量布局优化，以进一步降低“以算换存”带来的额外时延。
  \item \textbf{在线/自适应的秩与子空间更新：} 目前对$W_k$和$W_v$的变换矩阵都是在校准集上离线计算得到的，并不能完全反应模型在下游任务推理过程中各层注意力分数和激活值的统计特征。因此我们未来的另一个研究目标是在不显著增加开销的前提下，引入在线统计与子空间更新机制，使变换矩阵与秩分配能够随输入上下文与任务形态自适应变化，提升跨域迁移与长对话场景下的稳健性。
  \item \textbf{与其他维度压缩的协同：} 将本文的特征维压缩与序列维度方法（丢弃/合并/检索式保留）、层间共享、以及更细粒度的混合精度策略协同组合，比如通过特征降维后的Key和Query更快地计算注意力来估计原本完整计算的注意力，以此更快地在序列维度挑选注意力分数高的重要词元。通过多维度压缩协同来探索在固定总预算下的最优“多维压缩配方”，并研究多策略叠加时的误差互补关系。
  %\item \textbf{分布式推理与通信优化：} 在张量并行、流水线并行或prefill/decoding分离等系统中，KV缓存跨设备传输成本显著。未来可进一步研究压缩KV在跨卡通信中的编码格式、传输与解码开销，以及与并行策略的联合优化。
\end{itemize}