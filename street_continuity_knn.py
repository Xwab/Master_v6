#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import pandas as pd
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ===================== 0) é…ç½® =====================
# è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è·¯å¾„
excel_path = "/home/users/yyb/last/567append_all.xlsx"
sheet_name = "Sheet1"

# === å¼€å…³: æ˜¯å¦ä½¿ç”¨ KNN é‚»å±…ç‰¹å¾ä½œä¸ºè¾“å…¥ ===
USE_KNN_FEATURES = True

out_cv_xlsx   = "/home/users/yyb/last/cv_residual_nn_knn_separated.xlsx"
out_pred_xlsx = "/home/users/yyb/last/completed_residual_nn_knn_separated.xlsx"

# åˆ›å»ºè¾“å‡ºç›®å½•
for p in [out_cv_xlsx, out_pred_xlsx]:
    try:
        Path(p).parent.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        print(f"Warning: Could not create directory for {p}: {e}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ Device: {device}")

# ===================== 1) è¯»å…¥æ•°æ® =====================
na_tokens = ["", " ", "  ", "\t", "\n", "NA", "N/A", "na", "n/a", ".", "None", "null", "NULL"]
try:
    df = pd.read_excel(excel_path, sheet_name=sheet_name, na_values=na_tokens, keep_default_na=True)
except FileNotFoundError:
    print(f"Error: File not found at {excel_path}. Please check the path.")
    # ç”Ÿæˆå‡æ•°æ®ç”¨äºæ¼”ç¤º (å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨)
    print("Generating dummy data for testing logic...")
    data = []
    names = [f"Street_{i}" for i in range(100)]
    for y in [2000, 2010, 2020]:
        for n in names:
            # æ¨¡æ‹Ÿéƒ¨åˆ†ç¼ºå¤±: 2020å¹´éƒ¨åˆ†targetç¼ºå¤±
            is_target_missing = (y == 2020) and (int(n.split('_')[1]) > 70)
            row = {"åç§°": n, "year": y, "area": 100}
            for v in ["age65sacle", "malesacle", "nong_incomesacle", "shengchan_incomesacle", 
                      "qita_incomesacle", "unemploysacle", "tizhisacle", "outsidesacle"]:
                row[v] = np.random.rand()
            for t in ["bungalowsacle", "nowcsacle", "nobathroomsacle", "nokitchensacle"]:
                row[t] = np.nan if is_target_missing else np.random.rand()
            # Non-sacle columns (not used in main logic but present in load)
            for t in ["bungalow", "nowc", "nobathroom", "nokitchen"]:
                row[t] = np.nan
            data.append(row)
    df = pd.DataFrame(data)

df["year"] = pd.to_numeric(df["year"], errors="coerce").astype("Int64")
df["åç§°"] = df["åç§°"].astype(str).str.strip()

# å®šä¹‰å˜é‡åˆ—è¡¨
# ç›®æ ‡å˜é‡ (Percentages)
target_base = ["bungalowsacle", "nowcsacle", "nobathroomsacle", "nokitchensacle"]
# åŸå§‹è®¡æ•°å˜é‡ (ç”¨äºæ¸…æ´—ï¼Œä¸ç›´æ¥å…¥æ¨¡)
target_base_no_sacle = ["bungalow", "nowc", "nobathroom", "nokitchen"]

# æ‰€æœ‰å…¥æ¨¡å˜é‡ (åŒ…å« Feature Base + Target Base)
# æ³¨æ„: feature_base æ˜¯æŒ‡é™¤ç›®æ ‡å¤–çš„ç‰¹å¾ (äººå£ã€ç»æµç­‰)
vars_all = [
    "area", "age65sacle", "malesacle",
    "nong_incomesacle", "shengchan_incomesacle", "qita_incomesacle",
    "unemploysacle", "tizhisacle", "outsidesacle",
    "bungalowsacle", "nowcsacle", "nobathroomsacle", "nokitchensacle"
    # æ³¨æ„: "bungalow" ç­‰åŸå§‹è®¡æ•°åˆ—ä¸åŒ…å«åœ¨è¿™é‡Œï¼Œé¿å…æ··æ·†
]

# feature_base: ç”¨ä½œ Input çš„ç‰¹å¾ (ä¸å«ç›®æ ‡å˜é‡)
feature_base = [v for v in vars_all if v not in target_base]

# ç±»å‹è½¬æ¢
cols_to_numeric = vars_all + target_base_no_sacle
for c in cols_to_numeric:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

# Wideè½¬æ¢: index=åç§°, columns=year
# ä¸ºäº†æ–¹ä¾¿å¤„ç†ï¼Œæˆ‘ä»¬å…ˆ pivot
wide = df.pivot_table(index="åç§°", columns="year", values=cols_to_numeric, aggfunc="first")
# Flatten columns
wide.columns = [f"{v}_{int(y)}" for (v, y) in wide.columns]
wide = wide.reset_index()

print("Data Loaded & Pivoted. Wide shape:", wide.shape)

# ===================== 2) ç­›é€‰è¿ç»­å­˜åœ¨çš„è¡—é“/ä¹¡é•‡ =====================
# é€»è¾‘ï¼š
# set1: åœ¨ 2000 å’Œ 2010 éƒ½å­˜åœ¨çš„
# set2: åœ¨ 2010 å’Œ 2020 éƒ½å­˜åœ¨çš„
# æ³¨æ„ï¼šä¸€ä¸ªè¡—é“å¯èƒ½åŒæ—¶å±äº set1 å’Œ set2ï¼Œè¿™æ²¡é—®é¢˜ï¼Œå®ƒä¼šè´¡çŒ®ä¸¤ä¸ªæ ·æœ¬ï¼ˆåˆ†åˆ«ç”¨äºä¸¤ä¸ªæ—¶æ®µï¼‰

def get_valid_names(df_wide, y1, y2, check_cols_prefix):
    """
    æ£€æŸ¥æŒ‡å®šå¹´ä»½æ˜¯å¦æœ‰æ•°æ®ã€‚
    è¿™é‡Œåªè¦ check_cols_prefix åœ¨å¯¹åº”å¹´ä»½åˆ—éå…¨ç©ºå³å¯åˆ¤å®šä¸º"å­˜åœ¨"ã€‚
    æˆ–è€…ç®€å•ç‚¹ï¼Œåªè¦ pivot åæœ‰å¯¹åº”çš„åˆ—ä¸”è¯¥è¡Œä¸å…¨ä¸º NaNã€‚
    ç”±äº pivot fill_value=NaNï¼Œæˆ‘ä»¬æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨ã€‚
    """
    # æ„å»ºè¯¥å¹´ä»½å¿…é¡»å­˜åœ¨çš„åˆ—ååˆ—è¡¨ (å– feature_base ä¸­çš„ä¸€ä¸ªä»£è¡¨å³å¯ï¼Œæˆ–è€…æ£€æŸ¥æ‰€æœ‰)
    # åªè¦ feature_base ä¸­çš„æ•°æ®å¤§éƒ¨åˆ†å­˜åœ¨å³å¯ã€‚ä¸ºäº†ä¸¥æ ¼èµ·è§ï¼Œæ£€æŸ¥æ‰€æœ‰ feature_base
    
    # ç®€å•çš„å­˜åœ¨æ€§æ£€æŸ¥ï¼šæ£€æŸ¥ feature_base åœ¨ y1 å’Œ y2 æ˜¯å¦å¤§ä½“å®Œæ•´
    # è¿™é‡Œæ”¾å®½ä¸€ç‚¹ï¼šåªè¦ feature_base çš„åˆ—åœ¨ dataframe é‡Œæœ‰å€¼ (not na)
    
    valid_indices = []
    for idx, row in df_wide.iterrows():
        # æ£€æŸ¥ y1
        has_y1 = all(pd.notna(row.get(f"{v}_{y1}", np.nan)) for v in feature_base)
        # æ£€æŸ¥ y2
        has_y2 = all(pd.notna(row.get(f"{v}_{y2}", np.nan)) for v in feature_base)
        
        if has_y1 and has_y2:
            valid_indices.append(row["åç§°"])
            
    return set(valid_indices)

names_00_10 = get_valid_names(wide, 2000, 2010, feature_base)
names_10_20 = get_valid_names(wide, 2010, 2020, feature_base)

print(f"Valid pairs 2000-2010: {len(names_00_10)}")
print(f"Valid pairs 2010-2020: {len(names_10_20)}")

# ===================== 3) å®šä¹‰å¤„ç†å‡½æ•°ï¼šå¤„ç†ä¸€ä¸ªæ—¶é—´æ®µ (T_prev -> T_curr) =====================
def process_period(wide_df, names_subset, year_prev, year_curr, is_training_data=True):
    """
    å¯¹æŒ‡å®šåå• names_subset å’Œå¹´ä»½å¯¹ (year_prev, year_curr) è¿›è¡Œå¤„ç†ï¼š
    1. æ„å»º KNN æœç´¢ç©ºé—´ (Search Vector: All_Vars_Prev + Feature_Base_Curr)
    2. è®¡ç®— KNN é‚»å±…å‡å€¼
    3. æ„å»ºæ ·æœ¬ç‰¹å¾ (Vars_Prev, Feature_Base_Curr, Delta, Neighbor_Feats)
    4. æ„å»ºç›®æ ‡ (Residuals)
    """
    
    # 1. ç­›é€‰æ•°æ®
    # åªå–åœ¨è¯¥æ—¶é—´æ®µå­˜åœ¨çš„è¡Œ
    sub_df = wide_df[wide_df["åç§°"].isin(names_subset)].copy()
    if sub_df.empty:
        return [], [], [], []
    
    sub_df = sub_df.reset_index(drop=True)
    
    # 2. å‡†å¤‡ KNN æœç´¢ç‰¹å¾
    # æœç´¢å‘é‡ = [æ‰€æœ‰ç‰¹å¾_{Prev}, éä½æˆ¿ç‰¹å¾_{Curr}]
    # ç†ç”±ï¼šç”¨"ä¹‹å‰çš„å…¨è²Œ" + "ç°åœ¨çš„éç›®æ ‡ç‰¹å¾" æ¥å¯»æ‰¾ç›¸ä¼¼è¡—é“
    search_cols = [f"{v}_{year_prev}" for v in vars_all] + \
                  [f"{v}_{year_curr}" for v in feature_base]
    
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ (é˜²æ­¢æŸäº›åˆ—åœ¨ pivot æ—¶ä¸¢å¤±)
    missing_cols = [c for c in search_cols if c not in sub_df.columns]
    if missing_cols:
        # å¦‚æœåˆ—ä¸å­˜åœ¨ï¼Œå¡«å……NaN
        for c in missing_cols:
            sub_df[c] = np.nan
            
    X_search = sub_df[search_cols].values
    
    # å¡«å…… + æ ‡å‡†åŒ– (ç”¨äº KNN)
    imputer_knn = SimpleImputer(strategy="median")
    X_search_imputed = imputer_knn.fit_transform(X_search)
    
    scaler_knn = StandardScaler()
    X_search_scaled = scaler_knn.fit_transform(X_search_imputed)
    X_search_scaled = np.nan_to_num(X_search_scaled, nan=0.0)
    
    # Fit KNN
    # n_neighbors å– 11 (åŒ…å«è‡ªå·±)
    k = min(11, len(sub_df))
    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X_search_scaled)
    _, indices = nbrs.kneighbors(X_search_scaled)
    
    # 3. æ„å»ºæ ·æœ¬
    X_samples = []
    Y_residuals = []
    Base_values = []
    Meta_info = [] # å­˜å‚¨åç§°ç­‰
    
    # é¢„å…ˆè·å–åˆ—ç´¢å¼•ä»¥åŠ é€Ÿ
    col_idx_map = {c: i for i, c in enumerate(sub_df.columns)}
    
    # å‡†å¤‡æ•°æ®çŸ©é˜µ
    data_matrix = sub_df.values
    
    for i in range(len(sub_df)):
        name = sub_df.iloc[i]["åç§°"]
        
        # --- KNN ç‰¹å¾è®¡ç®— ---
        neighbor_idxs = indices[i, 1:] # æ’é™¤è‡ªå·±
        if len(neighbor_idxs) == 0:
             # Fallback if k is small (e.g. only 1 sample)
            neighbor_idxs = [i]

        nbr_feats = []
        
        # é‚»å±…ç‰¹å¾ 1: Prev Year Mean (All Vars)
        # è®¡ç®—é‚»å±…åœ¨ year_prev çš„ vars_all å‡å€¼
        for v in vars_all:
            col_name = f"{v}_{year_prev}"
            if col_name in col_idx_map:
                vals = data_matrix[neighbor_idxs, col_idx_map[col_name]]
                nbr_feats.append(np.nanmean(vals.astype(float)))
            else:
                nbr_feats.append(0.0)
                
        # é‚»å±…ç‰¹å¾ 2: Curr Year Mean (Feature Base Only)
        # è®¡ç®—é‚»å±…åœ¨ year_curr çš„ feature_base å‡å€¼
        # æ³¨æ„ï¼šä¸ä½¿ç”¨é‚»å±…çš„ target (housing) _currï¼Œä»¥ä¿æŒä¸€è‡´æ€§(é¢„æµ‹æ—¶ä¹Ÿæ²¡æœ‰)
        # é™¤éç¡®å®šä½œä¸º spatial lagã€‚è¿™é‡Œæ ¹æ®"ç”¨10å¹´å‰...è®¡ç®—KNN...æ±‚KNNé‚»å±…å‡å€¼"ï¼Œ
        # ä¿å®ˆèµ·è§ä½¿ç”¨ input features çš„å‡å€¼ã€‚
        for v in feature_base:
            col_name = f"{v}_{year_curr}"
            if col_name in col_idx_map:
                vals = data_matrix[neighbor_idxs, col_idx_map[col_name]]
                nbr_feats.append(np.nanmean(vals.astype(float)))
            else:
                nbr_feats.append(0.0)
        
        # --- æ ·æœ¬è‡ªèº«ç‰¹å¾ ---
        sample_feats = []
        
        # A. å†å²å…¨é‡ (Prev)
        prev_vals_dict = {}
        for v in vars_all:
            col_name = f"{v}_{year_prev}"
            val = sub_df.at[i, col_name] if col_name in sub_df.columns else np.nan
            sample_feats.append(val)
            prev_vals_dict[v] = val
            
        # B. å½“å‰éç›®æ ‡ (Curr)
        curr_vals_dict = {}
        for v in feature_base:
            col_name = f"{v}_{year_curr}"
            val = sub_df.at[i, col_name] if col_name in sub_df.columns else np.nan
            sample_feats.append(val)
            curr_vals_dict[v] = val
            
        # C. å˜åŒ–é‡ (Delta)
        for v in feature_base:
            p = prev_vals_dict.get(v, np.nan)
            c = curr_vals_dict.get(v, np.nan)
            if pd.notna(p) and pd.notna(c):
                sample_feats.append(c - p)
            else:
                sample_feats.append(np.nan)
                
        # D. KNN é‚»å±…ç‰¹å¾
        if USE_KNN_FEATURES:
            sample_feats.extend(nbr_feats)
        
        # --- Target (Residuals) ---
        # Base value = Prev year target
        base_vals = []
        for t in target_base:
            base_vals.append(prev_vals_dict.get(t, np.nan))
            
        # Residual = Curr - Prev
        # ä»…å½“æˆ‘ä»¬éœ€è¦ label æ—¶è®¡ç®— (training data)
        # ä¸”å¿…é¡»å½“å‰ä¹Ÿæœ‰å€¼
        residuals = []
        has_labels = True
        
        if is_training_data: # æˆ–è€…æ˜¯æ„å»º potential training sample
            # æ£€æŸ¥æ˜¯å¦æœ‰ Target Label
            for idx_t, t in enumerate(target_base):
                # ä¿®æ”¹ï¼šæ ¹æ® raw count (target_base_no_sacle) æ˜¯å¦ä¸º NaN æ¥åˆ¤æ–­æ˜¯å¦ç¼ºå¤±
                # æ³¨æ„ï¼štarget_base_no_sacle å¿…é¡»ä¸ target_base é¡ºåºå¯¹åº”
                t_raw = target_base_no_sacle[idx_t]
                col_name_raw = f"{t_raw}_{year_curr}"
                val_raw = sub_df.at[i, col_name_raw] if col_name_raw in sub_df.columns else np.nan

                col_name = f"{t}_{year_curr}"
                val_curr = sub_df.at[i, col_name] if col_name in sub_df.columns else np.nan
                val_prev = base_vals[idx_t]
                
                # åˆ¤å®šæ¡ä»¶ï¼šRaw Count éç©º (è¡¨ç¤ºå½“å‰å¹´ä»½æ•°æ®æœ‰æ•ˆï¼Œå“ªæ€• val_curr æ˜¯ 0) ä¸” Prev éç©º
                if pd.notna(val_raw) and pd.notna(val_prev):
                    residuals.append(val_curr - val_prev)
                else:
                    residuals.append(np.nan)
                    has_labels = False # åªè¦æœ‰ä¸€ä¸ªtargetç¼ºå¤±ï¼Œå°±è§†ä¸ºæ— å®Œæ•´label
        
        # æ”¶é›†ç»“æœ
        # æ³¨æ„ï¼šé¢„æµ‹é›†ä¸éœ€è¦ has_labels ä¸ºçœŸ
        X_samples.append(sample_feats)
        Y_residuals.append(residuals if is_training_data else [np.nan]*4)
        Base_values.append(base_vals)
        Meta_info.append({"åç§°": name, "Period": f"{year_prev}->{year_curr}", "HasLabel": has_labels})

    return X_samples, Y_residuals, Base_values, Meta_info


# ===================== 4) æ„å»ºæ•°æ®é›† =====================
print("Processing Period 1: 2000 -> 2010...")
X1, Y1, Base1, Meta1 = process_period(wide, names_00_10, 2000, 2010, is_training_data=True)

print("Processing Period 2: 2010 -> 2020...")
# è¿™é‡Œæˆ‘ä»¬å¯¹æ‰€æœ‰ 2010-2020 å­˜åœ¨çš„è¡—é“éƒ½è¿›è¡Œå¤„ç†ï¼Œä¹‹åå†æ ¹æ®æ˜¯å¦æœ‰ Label åˆ†ä¸º Train/Pred
X2, Y2, Base2, Meta2 = process_period(wide, names_10_20, 2010, 2020, is_training_data=True)

# åˆå¹¶å¹¶æ‹†åˆ† Train / Pred
X_train_list = []
Y_train_list = []
Base_train_list = []
Meta_train_list = []

X_pred_list = []
Base_pred_list = []
Meta_pred_list = []

# å¤„ç† Period 1 (2000->2010): åªè¦æœ‰ Label å°±è¿›è®­ç»ƒé›†
for i, meta in enumerate(Meta1):
    if meta["HasLabel"] and not np.isnan(Y1[i]).any():
        X_train_list.append(X1[i])
        Y_train_list.append(Y1[i])
        Base_train_list.append(Base1[i])
        Meta_train_list.append(meta)

# å¤„ç† Period 2 (2010->2020): æœ‰ Label -> è®­ç»ƒé›†, æ—  Label -> é¢„æµ‹é›†
for i, meta in enumerate(Meta2):
    if meta["HasLabel"] and not np.isnan(Y2[i]).any():
        X_train_list.append(X2[i])
        Y_train_list.append(Y2[i])
        Base_train_list.append(Base2[i])
        Meta_train_list.append(meta)
    else:
        # å¦‚æœæ˜¯ 2010->2020 ä¸”ç¼ºå¤± Labelï¼Œåˆ™æ˜¯æˆ‘ä»¬éœ€è¦é¢„æµ‹çš„ç›®æ ‡
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾è¿™å°±æ˜¯ç›®æ ‡ä»»åŠ¡ (é¢„æµ‹ 2020 ç¼ºå¤±å€¼)
        X_pred_list.append(X2[i])
        Base_pred_list.append(Base2[i])
        Meta_pred_list.append(meta)

X_train_raw = np.array(X_train_list, dtype=np.float32)
Y_train_res_raw = np.array(Y_train_list, dtype=np.float32)

X_pred_raw = np.array(X_pred_list, dtype=np.float32)
Pred_Base_Y = np.array(Base_pred_list, dtype=np.float32)

print(f"Dataset Constructed.")
print(f"Using KNN Features: {USE_KNN_FEATURES}")
print(f"Train samples: {len(X_train_raw)} (From 2000->2010 and 2010->2020)")
print(f"Pred samples:  {len(X_pred_raw)} (From 2010->2020 Missing Labels)")

# ===================== 5) é¢„å¤„ç† (Impute + Scale) =====================
imputer = SimpleImputer(strategy="median")
# è®­ç»ƒé›† fit
X_train_imputed = imputer.fit_transform(X_train_raw)
# é¢„æµ‹é›† transform
if len(X_pred_raw) > 0:
    X_pred_imputed = imputer.transform(X_pred_raw)
else:
    X_pred_imputed = X_pred_raw

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
if len(X_pred_raw) > 0:
    X_pred_scaled = scaler.transform(X_pred_imputed)
else:
    X_pred_scaled = X_pred_raw

# ===================== 6) æ¨¡å‹å®šä¹‰ =====================
class RobustNet(nn.Module):
    def __init__(self, input_dim):
        super(RobustNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2), 
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 4)
        )
        
    def forward(self, x):
        return self.net(x)

def train_one_model(X, y, seed, epochs=800, lr=0.001):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        
    X_t = torch.tensor(X, dtype=torch.float32).to(device)
    y_t = torch.tensor(y, dtype=torch.float32).to(device)
    
    model = RobustNet(X.shape[1]).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)
    criterion = nn.MSELoss()
    
    model.train()
    for e in range(epochs):
        optimizer.zero_grad()
        pred = model(X_t)
        loss = criterion(pred, y_t)
        
        if torch.isnan(loss):
            print(f"âš ï¸ NaN Loss at epoch {e}. Break.")
            break
            
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
    model.eval()
    return model

def predict_ensemble(models, X):
    if len(X) == 0:
        return np.array([])
    X_t = torch.tensor(X, dtype=torch.float32).to(device)
    preds = []
    with torch.no_grad():
        for m in models:
            preds.append(m(X_t).cpu().numpy())
    return np.mean(preds, axis=0)

# ===================== 7) CV è¯„ä¼° =====================
print("\nRunning CV...")
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
Y_cv_pred_res = np.zeros_like(Y_train_res_raw)
Base_Y_train_arr = np.array(Base_train_list)

# è®°å½• CV ç»“æœ
metrics_rows = []

if len(X_train_scaled) >= 5:
    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_scaled)):
        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]
        y_tr, y_val = Y_train_res_raw[train_idx], Y_train_res_raw[val_idx]
        
        models = []
        for i in range(5):
            m = train_one_model(X_tr, y_tr, seed=fold*10+i)
            models.append(m)
            
        y_pred_val = predict_ensemble(models, X_val)
        Y_cv_pred_res[val_idx] = y_pred_val

    # è¿˜åŸ
    Y_cv_pred_final = Base_Y_train_arr + Y_cv_pred_res
    Y_cv_pred_final = np.clip(Y_cv_pred_final, 0, 1)
    Y_true_final = Base_Y_train_arr + Y_train_res_raw

    for j, t in enumerate(target_base):
        mae = mean_absolute_error(Y_true_final[:, j], Y_cv_pred_final[:, j])
        rmse = np.sqrt(mean_squared_error(Y_true_final[:, j], Y_cv_pred_final[:, j]))
        r2 = r2_score(Y_true_final[:, j], Y_cv_pred_final[:, j])
        bias = np.mean(Y_cv_pred_final[:, j] - Y_true_final[:, j])
        
        metrics_rows.append({"Target": t, "MAE": mae, "RMSE": rmse, "R2": r2, "Bias": bias})

    print(pd.DataFrame(metrics_rows).round(5))
    
    # ä¿å­˜ CV ç»“æœ
    try:
        with pd.ExcelWriter(out_cv_xlsx, engine="openpyxl") as writer:
            pd.DataFrame(metrics_rows).to_excel(writer, sheet_name="metrics", index=False)
            # ä¿å­˜è¯¦ç»†é¢„æµ‹å¯¹æ¯”
            cv_detail = pd.DataFrame(Y_cv_pred_final, columns=[f"Pred_{t}" for t in target_base])
            for j, t in enumerate(target_base):
                cv_detail[f"True_{t}"] = Y_true_final[:, j]
            # æ·»åŠ  Meta ä¿¡æ¯
            meta_df = pd.DataFrame(Meta_train_list)
            cv_detail = pd.concat([meta_df.reset_index(drop=True), cv_detail], axis=1)
            cv_detail.to_excel(writer, sheet_name="cv_details", index=False)
    except Exception as e:
        print(f"Error saving CV results: {e}")
else:
    print("Not enough samples for CV.")

# ===================== 8) æœ€ç»ˆè¡¥å…¨ (Prediction) =====================
print("\nFinal Prediction on missing 2020 data...")

if len(X_pred_scaled) > 0:
    final_models = []
    for i in range(5):
        m = train_one_model(X_train_scaled, Y_train_res_raw, seed=100+i)
        final_models.append(m)

    pred_res = predict_ensemble(final_models, X_pred_scaled)

    # è¿˜åŸ: Y_2020 = Y_2010 (Base) + Delta
    Y_pred_final = Pred_Base_Y + pred_res
    Y_pred_final = np.clip(Y_pred_final, 0, 1)

    # å¡«å›åŸå§‹ Wide è¡¨ (æˆ–è€…åˆ›å»ºä¸€ä¸ªæ–°çš„ç»“æœè¡¨)
    # æˆ‘ä»¬åŸºäº wide è¡¨åšä¸€ä»½æ‹·è´
    completed_df = wide.copy()
    
    # æ„å»ºé¢„æµ‹ç»“æœå­—å…¸: {åç§°: [pred_val1, pred_val2, ...]}
    pred_map = {}
    for i, meta in enumerate(Meta_pred_list):
        name = meta["åç§°"]
        pred_map[name] = Y_pred_final[i]
        
    # å¡«å……
    fill_count = 0
    for idx, row in completed_df.iterrows():
        name = row["åç§°"]
        if name in pred_map:
            preds = pred_map[name]
            fill_count += 1
            for j, t in enumerate(target_base):
                col_2020 = f"{t}_2020"
                # å³ä½¿åŸä½ç½®æœ‰å€¼(ç†è®ºä¸Šä¸åº”è¯¥ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨Pred listé‡Œ)ï¼Œä¹Ÿè¦†ç›–æˆ–å¡«å……
                # æ ¹æ®é¢˜æ„æ˜¯å¡«å……ç¼ºå¤±çš„
                if pd.isna(completed_df.at[idx, col_2020]):
                    completed_df.at[idx, col_2020] = preds[j]

    print(f"Filled {fill_count} streets with predictions.")

    try:
        with pd.ExcelWriter(out_pred_xlsx, engine="openpyxl") as writer:
            completed_df.to_excel(writer, sheet_name="wide_completed", index=False)
        print(f"Saved completed file to {out_pred_xlsx}")
    except Exception as e:
        print(f"Error saving prediction results: {e}")

else:
    print("No prediction samples found.")

print("âœ… Done. Continuity-Filtered KNN + Residual NN Ensemble.")
