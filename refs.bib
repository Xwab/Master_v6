@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{xiaoefficient,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@inproceedings{han2024lm,
  title={Lm-infinite: Zero-shot extreme length generalization for large language models},
  author={Han, Chi and Wang, Qifan and Peng, Hao and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={3991--4008},
  year={2024}
}

@article{zhang2023h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}

@article{liu2023scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={52342--52364},
  year={2023}
}

@inproceedings{oren2024transformers,
  title={Transformers are Multi-State RNNs},
  author={Oren, Matanel and Hassid, Michael and Yarden, Nir and Adi, Yossi and Schwartz, Roy},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={18724--18741},
  year={2024}
}


@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={22947--22970},
  year={2024}
}

@article{cai2024pyramidkv,
  title={Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling},
  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Li, Yucheng and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Hu, Junjie and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

@inproceedings{yang2024pyramidinfer,
  title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference},
  author={Yang, Dongjie and Han, Xiaodong and Gao, Yan and Hu, Yao and Zhang, Shilin and Zhao, Hai},
  booktitle={ACL (Findings)},
  year={2024}
}


@article{feng2024ada,
  title={Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference},
  author={Feng, Yuan and Lv, Junlin and Cao, Yukun and Xie, Xike and Zhou, S Kevin},
  journal={arXiv preprint arXiv:2407.11550},
  year={2024}
}

@article{jo2025fastkv,
  title={FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation},
  author={Jo, Dongwon and Song, Jiwon and Kim, Yulhwa and Kim, Jae-Joon},
  journal={arXiv preprint arXiv:2502.01068},
  year={2025}
}

@article{liu2025chunkkv,
  title={Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference},
  author={Liu, Xiang and Tang, Zhenheng and Dong, Peijie and Li, Zeyu and Liu, Yue and Li, Bo and Hu, Xuming and Chu, Xiaowen},
  journal={arXiv preprint arXiv:2502.00299},
  year={2025}
}

@inproceedings{zhang2024cam,
  title={Cam: Cache merging for memory-efficient llms inference},
  author={Zhang, Yuxin and Du, Yuxuan and Luo, Gen and Zhong, Yunshan and Zhang, Zhenyu and Liu, Shiwei and Ji, Rongrong},
  booktitle={Forty-first international conference on machine learning},
  year={2024}
}

@inproceedings{yuan2025weightedkv,
  title={WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large Language Models},
  author={Yuan, Jian and He, Ziwei and Bai, Haoli and Leng, Jingwen and Jiang, Bo},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2025},
  organization={IEEE}
}

@inproceedings{cai2024lococo,
  title={LoCoCo: dropping in convolutions for long context compression},
  author={Cai, Ruisi and Tian, Yuandong and Wang, Zhangyang and Chen, Beidi},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={5348--5359},
  year={2024}
}

@article{brandon2024reducing,
  title={Reducing transformer key-value cache size with cross-layer attention},
  author={Brandon, William and Mishra, Mayank and Nrusimha, Aniruddha and Panda, Rameswar and Ragan-Kelley, Jonathan},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={86927--86957},
  year={2024}
}

@article{yang2024lossless,
  title={Lossless kv cache compression to 2\%},
  author={Yang, Zhen and Han, JN and Wu, Kan and Xie, Ruobing and Wang, An and Sun, Xingwu and Kang, Zhanhui},
  journal={arXiv preprint arXiv:2410.15252},
  year={2024}
}


@inproceedings{liu2024lckv,
  title={LCKV: Learner-Cleaner Optimized Adaptive Key-Value Separated LSM-Tree Store},
  author={Liu, Mingxuan and Gu, Jianhua and Zhao, Tianhai},
  booktitle={2024 IEEE 42nd International Conference on Computer Design (ICCD)},
  pages={479--482},
  year={2024},
  organization={IEEE}
}


@inproceedings{linqserve,
  title={QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving},
  author={Lin, Yujun and Tang, Haotian and Yang, Shang and Zhang, Zhekai and Xiao, Guangxuan and Gan, Chuang and Han, Song},
  booktitle={Eighth Conference on Machine Learning and Systems}
}


@article{ashkboos2024quarot,
  title={Quarot: Outlier-free 4-bit inference in rotated llms},
  author={Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Cameron, Pashmina and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={100213--100240},
  year={2024}
}

@article{zhao2024atom,
  title={Atom: Low-bit quantization for efficient and accurate llm serving},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={196--209},
  year={2024}
}


@inproceedings{liu2024kivi,
  title={KIVI: a tuning-free asymmetric 2bit quantization for KV cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={32332--32344},
  year={2024}
}

@article{hooper2024kvquant,
  title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun S and Keutzer, Kurt and Gholami, Amir},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={1270--1303},
  year={2024}
}


@inproceedings{cheng2025qaq,
  title={Qaq: Quality adaptive quantization for llm kv cache},
  author={Cheng, Wen and Dong, Shichen and Qin, Jiayu and Wang, Wei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2542--2550},
  year={2025}
}


@inproceedings{hanlogquant,
  title={LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation},
  author={Han, CHEN and Jiang, Zicong and Zhang, Zining and He, Bingsheng and Pingyi, Luo and Lu, Mian and Chen, Yuqiang},
  booktitle={Sparsity in LLMs (SLLM): Deep Dive into Mixture of Experts, Quantization, Hardware, and Inference}
}


@article{zhang2024kv,
  title={Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization},
  author={Zhang, Tianyi and Yi, Jonah and Xu, Zhaozhuo and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={3304--3331},
  year={2024}
}



@inproceedings{zandieh2025qjl,
  title={Qjl: 1-bit quantized jl transform for kv cache quantization with zero overhead},
  author={Zandieh, Amir and Daliri, Majid and Han, Insu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={24},
  pages={25805--25813},
  year={2025}
}


@article{son2025nsnquant,
  title={NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache},
  author={Son, Donghyun and Choi, Euntae and Yoo, Sungjoo},
  journal={arXiv preprint arXiv:2505.18231},
  year={2025}
}

@article{yuan2023asvd,
  title={Asvd: Activation-aware singular value decomposition for compressing large language models},
  author={Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Yang, Dawei and Wu, Qiang and Yan, Yan and Sun, Guangyu},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}


@inproceedings{zhang2024lorc,
  title={LORC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy},
  author={Zhang, Rongzhi and Wang, Kuan and Liu, Liyuan and Wang, Shuohang and Cheng, Hao and Zhang, Chao and others},
  booktitle={Workshop on Machine Learning and Compression, NeurIPS 2024}
}

@inproceedings{wang2024cskv,
  title={CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios},
  author={Wang, Luning and Li, Shiyao and Ning, Xuefei and Yuan, Zhihang and Yan, Shengen and Dai, Guohao and Wang, Yu},
  booktitle={NeurIPS Efficient Natural Language and Speech Processing Workshop},
  pages={468--484},
  year={2024},
  organization={PMLR}
}


@inproceedings{chang2025palu,
  title={Palu: KV-Cache Compression with Low-Rank Projection},
  author={Chang, Chi-Chih and Lin, Wei-Cheng and Lin, Chien-Yu and Chen, Chong-Yan and Hu, Yu-Fang and Wang, Pei-Shuo and Huang, Ning-Chi and Ceze, Luis and Abdelfattah, Mohamed S and Wu, Kai-Chiang},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}

@inproceedings{saxena2024eigen,
  title={Eigen Attention: Attention in Low-Rank Space for KV Cache Compression},
  author={Saxena, Utkarsh and Saha, Gobinda and Choudhary, Sakshi and Roy, Kaushik},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={15332--15344},
  year={2024}
}

@inproceedings{linmatryoshkakv,
  title={MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection},
  author={Lin, Bokai and Zeng, Zihao and Xiao, Zipeng and Kou, Siqi and Hou, TianQi and Gao, Xiaofeng and Zhang, Hao and Deng, Zhijie},
  booktitle={The Thirteenth International Conference on Learning Representations}
}

@article{zhu2025ojakv,
  title={OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule},
  author={Zhu, Yuxuan and Yang, David H and Amiri, Mohammad Mohammadi and Murugesan, Keerthiram and Pedapati, Tejaswini and Chen, Pin-Yu},
  journal={arXiv preprint arXiv:2509.21623},
  year={2025}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv e-prints},
  pages={arXiv--2407},
  year={2024}
}

@article{team2024qwen2,
  title={Qwen2 technical report},
  author={Team, Qwen and others},
  journal={arXiv preprint arXiv:2407.10671},
  volume={2},
  number={3},
  year={2024}
}

@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and De Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{marcus-etal-1993-building, title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank", author = "Marcus, Mitchell P. and Santorini, Beatrice and Marcinkiewicz, Mary Ann", journal = "Computational Linguistics", volume = "19", number = "2", year = "1993", url = "https://www.aclweb.org/anthology/J93-2004", pages = "313--330", }

@inproceedings{clark2019boolq,
  title =     {BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author =    {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei, and Kwiatkowski, Tom and Collins, Michael, and Toutanova, Kristina},
  booktitle = {NAACL},
  year =      {2019},
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@article{allenai:arc,
      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
                    Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
      journal   = {arXiv:1803.05457v1},
      year      = {2018},
}

@inproceedings{Bisk2020,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}

@article{huang2023ceval,
title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models}, 
author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and Fu, Yao and Sun, Maosong and He, Junxian},
journal={arXiv preprint arXiv:2305.08322},
year={2023}
}

@misc{bai2023longbench,
      title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding}, 
      author={Yushi Bai and Xin Lv and Jiajie Zhang and Hongchang Lyu and Jiankai Tang and Zhidian Huang and Zhengxiao Du and Xiao Liu and Aohan Zeng and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
      year={2023},
      eprint={2308.14508},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{mousi2024aradicebenchmarksdialectalcultural,
      title={{AraDiCE}: Benchmarks for Dialectal and Cultural Capabilities in LLMs},
      author={Basel Mousi and Nadir Durrani and Fatema Ahmad and Md. Arid Hasan and Maram Hasanain and Tameem Kabbani and Fahim Dalvi and Shammur Absar Chowdhury and Firoj Alam},
      year={2024},
      publisher={arXiv:2409.11404},
      url={https://arxiv.org/abs/2409.11404},
}


@InProceedings{ai2:winogrande,
title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
authors={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi
},
year={2019}
}

@inproceedings{wangsvd,
  title={SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression},
  author={Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
  booktitle={The Thirteenth International Conference on Learning Representations}
}

@inproceedings{xuthink,
  title={ThinK: Thinner Key Cache by Query-Driven Pruning},
  author={Xu, Yuhui and Jie, Zhanming and Dong, Hanze and Wang, Lei and Lu, Xudong and Zhou, Aojun and Saha, Amrita and Xiong, Caiming and Sahoo, Doyen},
  booktitle={The Thirteenth International Conference on Learning Representations}
}


@Comment{jabref-meta: databaseType:biblatex;}
